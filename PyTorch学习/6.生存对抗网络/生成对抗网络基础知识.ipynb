{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成对抗网络基本原理\n",
    "## 生成式建模概述\n",
    "- 生成问题（图像、文本、语音）是人工智能领域重要的一个分支。它能实现模拟未来、处理缺失数据、多模态输出、解决真实的数据生产问题。自回归模型的优点是简单、稳定的训练过程，缺点是采样效率低。变分自编码器（VAE）通过训练数据学习到的是参数的概率分布，其优点是可以带变量的概率图模型学习与贝叶斯推断，缺点是生成样本模糊。虽然这些模型可以生成一定数量的图片，但是与我们期待的结果还是有一定的差距，发展缓慢且瓶颈比较明显，于是，在2014年，Goodfellow在NIPS会议上发表了一篇名为《Generative Adversarial Nets》的论文，首次将GAN网络带进人们的视野。然而标准的GAN如DCGAN等并不能控制生成的图片的效果，条件GAN（CGAN）则使用了条件控制变量作为输入，是几乎后续所有性能强大的GAN的基础。\n",
    "\n",
    "## GAN的优缺点\n",
    "- 优点\n",
    "    - GAN是一种生成式模型，相比较其他生成模型（玻尔兹曼机和GSNs）只用到了反向传播，而不需要复杂的马尔科夫链。\n",
    "    - 相比其他所有模型, GAN可以产生更加清晰，真实的样本。\n",
    "    - GAN采用的是一种无监督的学习方式训练，可以被广泛用在无监督学习和半监督学习领域。\n",
    "    - 相比于变分自编码器, GANs没有引入任何决定性偏置( deterministic bias),变分方法引入决定性偏置,因为他们优化对数似然的下界，而不是似然度本身,这看起来导致了VAEs生成的实例比GANs更模糊。\n",
    "    - 相比VAE, GANs没有变分下界,如果判别器训练良好，那么生成器可以完美的学习到训练样本的分布.换句话说，GANs是渐进一致的,但是VAE是有偏差的。\n",
    "    - GAN应用到一些场景上，比如图片风格迁移，超分辨率，图像补全，去噪，避免了损失函数设计的困难。\n",
    "- 缺点\n",
    "    - 训练GAN需要达到纳什均衡，有时候可以用梯度下降法做到，有时候做不到。我们还没有找到很好的达到纳什均衡的方法，所以训练GAN相比VAE或者PixelRNN是不稳定的。\n",
    "    - GAN不适合处理离散形式的数据，比如文本。\n",
    "    - GAN存在训练不稳定、梯度消失、模式崩溃的问题（目前已解决）。\n",
    "\n",
    "## GAN的应用和发展\n",
    "- 随着人工智能时代的到来，下一代媒体将由人工智能驱动，人工智能可能给数字内容领域带来重塑。近年来，人工智能合成内容正在快速兴起，利用AI算法生产、修改数据和信息内容，从而让文字、音乐、图像、语音、视频等都可由AI自动生产。尤其是随着生产对抗网络这一AI算法的诞生，AI生成内容中的“深度合成”技术可以实现换脸、人脸合成、语音合成、视频生成甚至数字虚拟人等诸多应用，获得了各界的广泛关注。2017年是深度合成技术进入大众视野的第一年，随后的两年里，随着GAN算法的发展应用和开源项目的增多，依托深度合成技术的诸多商业化应用问世，技术应用潜力逐渐显现。与此同时，深度合成技术强大的仿真能力也引发对技术作恶和技术滥用的担忧，例如金融诈骗、色情复仇、隐私侵权、商业诋毁，乃至威胁国家安全、公共安全。商业环境中，一方面，deepnude等应用程序因为涉嫌侵权被下架或阉割，充斥着色情合成内容等黑灰产的Reddit论坛deepfake被关闭；另一方面，开源工具受到欢迎，诸多AI初创公司试水深度合成工具的开发，相继寻找下一个能引发爆点的应用，在通讯、社交、电影、网络购物等领域的尝试令人惊喜。回顾技术的发展历程可以发现，经历过技术初问世时的狂热追捧以及随之而来的“威胁论”和“恐慌论”，社会对深度合成技术及相关应用的认识逐渐趋于理性，而2020年也有望成为深度合成技术走向大规模商业化应用的元年。总之，深度合成技术是人工智能发展到一定阶段的产物，它不会让社会真相失守，更不是世界秩序的威胁者。面对新技术的挑战，政府和监管者应当包容审慎，避免阻碍深度合成技术的有益的、创新性的应用，通过法律、技术、行业、用户的多重治理将其纳入可控的发展轨道。\n",
    "- “深度合成”依赖于人工智能技术，尤其是可以从大量数据中自主学习的深度学习算法模型。“深度合成”背后的AI技术主要包括自动编码器和生成对抗网络。自动编码器是一个人工神经网络，被训练来对输入数据进行重建以实现数据合成。GAN由两组相互对抗的人工神经网络组成，其中一个网络负责生成数据，另一个网络负责甄别。具体而言，在GAN的两个机器学习系统中，生成网络或者说生成器负责制作复制了原始数据集特征的合成数据如图片、音频记录、视频等，鉴别网络或者说鉴别器则负责识别合成的数据。基于每次迭代的结果，生成网络不断进行调整以创造越来越逼真、越来越接近于原始数据的新数据。生成器与鉴别器的竞争往往需要数千次或数百万次迭代，这让生成器不断改善其性能，直至让鉴别器不再能够区分真实数据和合成数据。最终，GAN可以对视频中的人脸进行高度逼真的渲染，从而合成高保真的信息内容。虽然GAN通常可以生成高度逼真的合成视频，但使用起来却更加困难且复杂。\n",
    "- 短期内，“深度合成”技术已经作用于影视、娱乐和社交等诸多领域，它们或是被用于升级传统的音视频处理或后期技术，带来更好的影音体验；或是被用来进一步打破语言障碍，优化社交体验。中长期来看，“深度合成”技术既可以基于其深度仿真的特征，超越时空限制，加深我们与虚拟世界的交互，也可以基于其合成性，创造一些超越真实世界的“素材”，比如合成数据，为科研或生产所用。具有而言，深度合成技术已开始在如下领域中使用，2020年更多商业化应用也将从这些领域涌现出来。\n",
    "- “深度合成”作为人工智能技术发展到一定阶段的产物，自2017年11月首次进入大众视野以来，经过过去两年的喧嚣和争议，随着技术的成熟，在2020年新的应用开始大量涌现，可谓是商业化应用元年。可以确定的是，深度合成技术在内容创意、营销、社交、娱乐、电商、通讯等诸多领域应用前景广阔，其未来应用令人期待。这也说明，深度合成并非关于“伪造”和“欺骗”技术，而是极富创造力和突破性的技术。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 算法内容\n",
    "## Part1 萌生想法\n",
    "- <font color = blue>对抗网络</font>希望通过改变<font color = red>参数</font>$\\theta$，使生成模型概率分布$p_{model}(x;\\theta)$能够逼近真实数据概率分布$p_{data}(x)$。  \n",
    "即通过$\\color{olive}{\\theta^*= \\arg\\max_{\\theta}L=\\arg\\max_{\\theta}\\prod_{i=1}^mp_{model}(x^{(i)};\\theta)}$使得$\\color{olive}{p_{model}(x;\\theta)}$逼近$\\color{olive}{p_{data}(x)}$。\n",
    "\n",
    "$\\begin{equation}\\begin{aligned}\n",
    "\\theta^*&=\\arg\\max_{\\theta}\\prod_{i=1}^mp_{model}(x^{(i)};\\theta)=\\arg\\max_{\\theta}\\color{teal}{\\log}\\prod_{i=1}^{m}p_{model}(x^{(i)};\\theta)=\\arg\\max_{\\theta}\\color{teal}{\\sum_{i=1}^{m}}\\log p_{model}(x^{(i)};\\theta)\\\\\n",
    "&=\\arg\\max_{\\theta}\\color{teal}{E_{x \\sim p_{data}}}\\log p_{model}(x;\\theta) \n",
    "= \\arg\\max_{\\theta}\\color{teal}{\\int}p_{data}(x)\\log p_{model}(x;\\theta)\\color{teal}{\\rm dx}\\\\\n",
    "&= \\arg\\max_{\\theta}(\\int p_{data}(x)\\log p_{model}(x;\\theta){\\rm d}x \\color{teal}{-\\int p_{data}(x)\\log p_{data}(x){\\rm d}x})\\\\\n",
    "&= \\arg\\max_{\\theta}\\int p_{data}(x)\\color{teal}{(}\\log p_{model}(x;\\theta)-\\log p_{data}(x)\\color{teal}{)}{\\rm d}x\\\\\n",
    "&= \\arg\\max_{\\theta}\\int p_{data}(x)\\log \\color{teal}{\\frac{p_{model}(x;\\theta)}{p_{data}(x)}}{\\rm d}x\n",
    "= \\arg\\max_{\\theta}(-\\int p_{data}(x)\\log \\frac{p_{data}(x)}{p_{model}(x;\\theta)}{\\rm d}x)\\\\\n",
    "&=\\arg\\color{teal}{\\min_{\\theta}}(\\int p_{data}(x)\\log \\frac{p_{data}(x)}{p_{model}(x;\\theta)}{\\rm d}x)\n",
    "=\\arg\\min_{\\theta}\\color{teal}{KL(p_{data}(x)||p_{model}(x))}\n",
    "\\end{aligned}\\end{equation}$  \n",
    "\n",
    "即$\\bbox[4px,border:2px solid red]\n",
    "{\\theta^*=\\arg\\min_{\\theta}KL(p_{data}(x)||p_{model}(x))}$\n",
    "\n",
    "- <font color = red>知识点tip</font>：  \n",
    "    - KL散度：一种计算概率分布之间相似程度的计算方法。\n",
    "    - 在假定为连续随机变量的前提下，且两个概率分布分别为$P$和$Q$，则$KL(P||Q)=\\int p(x)\\log \\frac{p(x)}{q(x)}{\\rm d}x$。\n",
    "    - <font color = orange>注意：非负性，~~对称性~~。</font>  \n",
    "\n",
    "## Part2 遇到问题\n",
    "- 问题：\n",
    "    - <font color = blue>生成模型</font> $\\color{blue}{x={\\rm G}(z)}$\n",
    "    - 输入数据的概率分布函数 <font color = blue>已知</font>\n",
    "    - <font color = blue>但</font> 生成函数是一种神经网络的形式 $\\nRightarrow P_{model}(x)\\nRightarrow L$\n",
    "- 解决：\n",
    "    - 生成对抗网络中的代价函数\n",
    "    - <font color = teal>判别器D</font>：${\\rm J}^{(D)}({\\rm D},{\\rm G})=-\\frac{1}{2} E_{x\\sim p_{data}}[\\log {\\rm D(x)}]-\\frac{1}{2} E_{z\\sim p_{z}}[\\log (1-{\\rm D}({\\rm G}(z)))] $\n",
    "    - <font color = teal>生成器G</font>：${\\rm J}^{(G)}({\\rm D},{\\rm G})=\\frac{1}{2} E_{x\\sim p_{data}}[\\log {\\rm D(x)}]+\\frac{1}{2} E_{z\\sim p_{z}}[\\log (1-{\\rm D}({\\rm G}(z)))] $\n",
    "    - 价值函数${\\rm V}({\\rm D},{\\rm G})$表示${\\rm J}^{(G)}$和${\\rm J}^{(D)}$：\n",
    "        - $\\bbox[5px,border:2px solid red]\n",
    "{{\\rm V}({\\rm D},{\\rm G})=E_{x\\sim p_{data}}[\\log {\\rm D}(x)] + E_{z\\sim p_{z}}[\\log (1-{\\rm D}({\\rm G}(z)))]} $\n",
    "        - ${\\rm J}^{(D)} = -\\frac{1}{2}{\\rm V}({\\rm D},{\\rm G})$\n",
    "        - ${\\rm J}^{(G)} = \\frac{1}{2}{\\rm V}({\\rm D},{\\rm G})$\n",
    "    - 在生成对抗网络中，我们要计算的纳什平衡点正是要寻找一个生成器G和判别器D，使得各自的代价函数最小，即希望找到了一个${\\rm V}({\\rm D},{\\rm G})$对于生成器来说最小而对判别器来说最大。公式：$\\arg\\min_{G}\\max_{D}{\\rm V}({\\rm D},{\\rm G})$。例：鞍点。  \n",
    "    \n",
    "## Part3 可行性证明\n",
    "- 当生成数据的分布$p_{g}(x)$趋近于真实数据分布$p_{data}(x)$时，D网络输出的概率${\\rm D}^{*}(x)$会趋近于$\\frac{1}{2}$。这也是最终希望达到的训练结果，这时候G和D网络也就达到了一个平衡状态。\n",
    "<img src=\"https://cdn.nlark.com/yuque/0/2020/png/1508544/1591097001901-599ecd3a-276f-4836-85d3-39fe01f717af.png\" style=\"zoom:80%\" />\n",
    "- $z$是随机噪声，$x$是训练数据中的真实数据data。\n",
    "- 分别求出理想的判别器${\\rm D}^{*}$和生成器${\\rm G}^{*}$：\n",
    "    - $\\color{navy}{{\\rm D}^{*}}=\\arg\\max_{{\\rm D}}{\\rm V}({\\rm D},{\\rm G}^{*})=\\arg\\min_{{\\rm G}}\\max_{{\\rm D}}{\\rm V}({\\rm D},{\\rm G})=\\arg\\min_{G}{\\rm V}({\\rm D}^{*},{\\rm G})=\\color{navy}{{\\rm G}^{*}}=\\color{olive}{\\frac{1}{2}}$  \n",
    "\n",
    "- <font color = teal>最优判别器</font>\n",
    "    - 假设生成器G是固定的，令${\\rm G}(z)=x$，  \n",
    "    则$\\begin{equation}\\begin{aligned}\n",
    "{\\rm V}({\\rm G},{\\rm D})&=E_{x\\sim p_{data}}[\\log {\\rm D}(x)] + E_{z\\sim p_{z}}[\\log (1-{\\rm D}({\\rm G}(z)))]\n",
    "=E_{x\\sim p_{data}}[\\log {\\rm D}(x)] + E_{\\color{teal}{x\\sim p_{g}}}[\\log (1-\\color{teal}{{\\rm D}(x)})]\\\\\n",
    "&=\\color{teal}{\\int}p_{data}(x)\\log {\\rm D}(x)\\color{teal}{{\\rm d}x}\\,+\\color{teal}{\\int}p_g{(x)}\\log (1-{\\rm D(x)})\\color{teal}{{\\rm d}x}\n",
    "=\\color{teal}{\\int}p_{data}(x)\\log {\\rm D}(x)\\,+p_g{(x)}\\log (1-{\\rm D(x)})\\color{teal}{{\\rm d}x}\n",
    "\\end{aligned}\\end{equation}$  \n",
    "希望对  $f(x)=p_{data}(x)\\log {\\rm D}(x)+p_{g}(x)\\log (1-{\\rm D}(x))$，无论$x$取何值都能<font color = orange>最大</font>。  \n",
    "$\\frac{{\\rm d}f(x)}{{\\rm d}{\\rm D}(x)}\n",
    "=\\frac{p_{data}(x)}{{\\rm D}(x)}-\\frac{p_{g}(x)}{1-{\\rm D(x)}}\n",
    "=0\n",
    "\\Rightarrow\n",
    "\\bbox[5px,border:2px solid red]\n",
    "{{\\rm D}^{*}(x)\n",
    "=\\frac{p_{data}(x)}{p_{data}(x)+p_{g}(x)}}\n",
    "\\in (0,1)$  \n",
    "- <font color = teal>最优生成器</font>\n",
    "    - 代入${\\rm D}^{*}(x)$，即判别器D是固定的，\n",
    "    $\\begin{equation}\\begin{aligned}\n",
    "{\\rm C}({\\rm G})&=\\max_{D}{\\rm V}({\\rm G},{\\rm D})\n",
    "={\\rm V}({\\rm G},{\\rm D}^{*})\\\\\n",
    "&=E_{x\\sim p_{data}}[\\log {\\rm D}^{*}(x)] + E_{z\\sim p_{z}}[\\log (1-{\\rm D}^{*}({\\rm G}(z)))]\\\\\n",
    "&=E_{x\\sim p_{data}}[\\log {\\rm D}^{*}(x)] + E_\\color{teal}{{x\\sim p_{g}}}[\\log (1-\\color{teal}{{\\rm D}^{*}(x)})]\\\\\n",
    "&=E_{x\\sim p_{data}}[\\log \\color{teal}{\\frac{p_{data}(x)}{p_{data}(x)+p_g{(x)}}}]\\,+E_{x\\sim p_{g}}[\\log \\color{teal}{\\frac{p_{g}(x)}{p_{data}(x)+p_{g}(x)}}]\\\\\n",
    "&=E_{x\\sim p_{data}}[\\log \\frac{p_{data}(x)\\color{teal}{/2}}{(p_{data}(x)+p_{g}(x))\\color{teal}{/2}}]\\,+E_{x\\sim p_{g}}[\\log \\frac{p_{g}(x)\\color{teal}{/2}}{(p_{data}(x)+p_{g}(x))\\color{teal}{/2}}]\\\\\n",
    "&=E_{x\\sim p_{data}}[\\log \\frac{p_{data}(x)}{(p_{data}(x)+p_{g}(x))/2}]\\,+\\color{teal}{E_{x\\sim p_{data}}[-\\log2]}\\,+E_{x\\sim p_{g}}[\\log \\frac{p_{g}(x)}{(p_{data}(x)+p_{g}(x))/2}]\\,+\\color{teal}{E_{x\\sim p_{g}}[-\\log2]}\\\\\n",
    "&=\\color{teal}{\\int} p_{data}(x)\\log \\frac{p_{data}(x)}{(p_{data}(x)+p_{g}(x))/2}\\color{teal}{{\\rm d}x}\\,+\\color{teal}{\\int} p_{g}(x)\\log \\frac{p_{g}(x)}{(p_{data}(x)+p_{g}(x))/2}\\color{teal}{{\\rm d}x}\\,-\\log(4)\\\\\n",
    "&=-\\log(4)\\,+KL\\left(p_{data}(x)\\parallel \\frac{p_{data}(x)+p_{g}(x)}\n",
    "{2}\\right)+KL\\left(p_{g}(x)\\parallel\\frac{p_{data}(x)+p_{g}(x)}{2}\\right)\\\\\n",
    "&=-\\log(4)\\,+2\\cdot JSD(p_{data}\\parallel p_{g})\n",
    "\\end{aligned}\\end{equation}$  \n",
    "即 $\\bbox[5px,border:2px solid red]\n",
    "{\n",
    "{\\rm C}({\\rm G})=-\\log(4)\\,+2\\cdot JSD(p_{data}\\parallel p_{g})\n",
    "}$  \n",
    "当且仅当$p_{data}=p_{g}$的时候，上式可以取得全局<font color = orange>最小</font>值${\\rm C}^{*}=-\\log(4)$。$\\color{blue}{\\Rightarrow {\\rm G}^{*}=\\arg\\min_{G}\\max_{D}{\\rm V}({\\rm G},{\\rm D})}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 练习题\n",
    "- 通过$\\theta^*=\\arg\\max_{\\theta}\\prod_{i=1}^mp_{model}(x^{(i)};\\theta)$求解出$\\theta^*=\\arg\\min_{\\theta}KL(p_{data}(x)||p_{model}(x))$。\n",
    "- 求价值函数，使模型期望达到纳什均衡状态。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "tianchi_metadata": {
   "competitions": [],
   "datasets": [],
   "description": "",
   "notebookId": "163742",
   "source": "dsw"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
