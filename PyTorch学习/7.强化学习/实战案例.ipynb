{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "36bb051b-2c81-4929-b5ba-5b434120bd77"
   },
   "source": [
    "- 代理人必须在两个动作之间做出决定-向左或向右移动推车-以使连接到它的杆保持直立。你可以在Gym网站上找到官方排行榜，里面包含各种算法以及可视化。\n",
    "\n",
    "<img src=\"https://cdn.nlark.com/yuque/0/2021/png/1508544/1614609216355-90b4e735-4cc7-43fb-b155-5ec541f81828.png\"/>\n",
    "\n",
    "- 当代理观察环境的当前状态并选择动作时，环境转换到新状态，并且还返回指示动作的后果的奖励。在此任务中，每增加一个时间步长的奖励为+1，如果杆落得太远或者推车距离中心超过2.4个单位，则环境终止。这意味着更好的表现场景将持续更长的时间，以及积累更大的回报。\n",
    "- CartPole任务的设计使得代理的输入是4个实际值，表示环境状态（位置、速度等）。然而，神经网络可以纯粹通过观察场景来解决任务，因此我们将使用以cart为中心的屏幕补丁作为输入。也因此如此，我们的结果与官方排行榜的结果无法直接比较。因为我们的任务要困难得多，而且不幸的是，这确实减慢了训练速度，因为我们必须渲染所有帧。\n",
    "- 严格地说，我们将状态显示为当前屏幕补丁与前一个补丁之间的差异。这将允许代理从一个图像中考虑杆的速度。\n",
    "- 使用`DQN`解决CartPole问题，移动小车使得车上的摆杆倒立起来。\n",
    "\n",
    "- 需要安装 python-opengl ，需要定制镜像，暂时没法解决，可以在colab上尝试\n",
    "https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/2b3f06b04b5e96e4772746c20fcb4dcc/reinforcement_q_learning.ipynb\n",
    "\n",
    "# 加载相关库\n",
    "- 首先，让我们导入所需的包。\n",
    "    - 环境（gym）\n",
    "    - 神经网络（torch.nn）\n",
    "    - 优化（torch.optim）\n",
    "    - 自动微分（torch.autograd）\n",
    "    - 视觉任务的实用程序（torchvision）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "uuid": "256d3d2b-bc32-4ae3-a361-fc1f7ff3e1d6"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "ff265f3b-a979-459d-8013-5cf757521a29"
   },
   "source": [
    "# 复现记忆（Replay Memory）\n",
    "- 我们将使用经验重播记忆来训练我们的DQN。它存储代理观察到的转换，允许我们之后重用此数据。通过随机抽样，转换构建相关的一个批次。已经表明经验重播记忆极大地稳定并改善了DQN训练程序。\n",
    "- 为此，我们需要两个阶段\n",
    "    - Transition：一个命名元组，表示我们环境中的单个转换。它实际上将（状态，动作）对映射到它们的（next_state，reward）结果，状态是屏幕差异图像\n",
    "    - ReplayMemory：有界大小的循环缓冲区，用于保存最近观察到的过渡。它还实现了一个`.sample()`方法，用于为训练选择随机batch的转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "uuid": "b4762791-10c4-4b8f-954f-1d8806612c00"
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "25483953-5ec6-4368-bebe-eec125d199e0"
   },
   "source": [
    "# Q网络\n",
    "- 我们的模型将是一个卷积神经网络，它接收当前和之前的屏幕补丁之间的差异。它有两个输出，分别代表$Q(s, left)$和$Q(s, right)$（其中s是网络的输入）。实际上，网络正在尝试预测在给定当前输入的情况下采取每个动作的预期回报。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "uuid": "adc0e6d9-4cf8-4281-bba7-df12fae1d66e"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # 线性输入连接的数量取决于conv2d层的输出，因此取决于输入图像的大小，因此请对其进行计算。\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    # 使用一个元素调用以确定下一个操作，或在优化期间调用batch\n",
    "    # 返回 tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "6da2b42a-5937-4414-88a6-9b4d5e5fb6ff"
   },
   "source": [
    "# 输入提取\n",
    "- 下面的代码是用于从环境中提取和处理渲染图像的实用程序。它使用了`torchvision`软件包，可以轻松构成图像变换。运行单元后，它将显示一个提取的示例补丁。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "855f7a2a-1b0b-4bc8-955e-97e1ef0e853f"
   },
   "outputs": [],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "def get_screen():\n",
    "    # gym要求的返回屏幕是400×600×3，但有时更大，如800×1200×3。\n",
    "    # 将其转换为torch order(CHW)。\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    # cart位于下半部分，因此不包括屏幕的顶部和底部。\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # 去掉边缘，使得我们有一个以cart为中心的方形图像\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # 转换为float类型，重新缩放，转换为torch张量\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # 调整大小并添加batch维度 (BCHW)\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "d1c56d13-8ffa-4de9-8b33-2924dd1e3386"
   },
   "source": [
    "# 训练\n",
    "## 超参数和实用程序\n",
    "- 这个单元实例化我们的模型及其优化器，并定义了一些实用程序：\n",
    "- `select_action`：将根据epsilon贪婪政策选择一项行动。简而言之，我们有时会使用我们的模型来选择动作，有时我们只会统一采样。选择随机操作的概率将从`EPS_START`开始，并将以指数方式向`EPS_END`衰减。`EPS_DECAY`控制衰减的速度。\n",
    "- `plot_durations`：帮助绘制episodes的持续时间，以及过去100个episodes的平均值（官方评估中使用的度量）。该图将位于包含主要训练循环的单元下方，并将在每个episodes后更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# 获取屏幕大小，以便我们可以根据AI gym返回的形状正确初始化图层。\n",
    "# 此时的典型尺寸接近3×40×90\n",
    "# 这是get_screen()中的限幅和缩小渲染缓冲区的结果\n",
    "init_screen = get_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# 从gym行动空间中获取行动数量\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1)将返回每行的最大列值\n",
    "            # 最大结果的第二列是找到最大元素的索引，因此我们选择具有较大预期奖励的行动。\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # 取100个episode的平均值并绘制它们\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # 暂停一下，以便更新图表\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "86d86ce8-24be-4cb9-8cd7-a649679c88a7"
   },
   "source": [
    "# 训练循环\n",
    "- 在这里，您可以找到执行优化的单个步骤的`optimize_model`函数。它首先对一个batch进行采样，将所有张量连接成一个整体，计算$Q(s_t,a_t)$和$V(s_{t+1})=\\max_aQ(s_{t+1},a)$，并将它们组合成我们的损失。通过定义，如果s是终端状态，则设置$V(s)=0$。我们还使用目标网络来计算$V(s_{t+1})$以增加稳定性。目标网络的权重在大多数时间保持冻结状态，但每隔一段时间就会更新策略网络的权重。这通常是一系列步骤，但为了简单起见，我们将使用episodes。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "d111082c-f65d-4dd5-89e3-15d9bf8b4436"
   },
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # 转置batch（有关详细说明，请参阅https://stackoverflow.com/a/19343/3343043）\n",
    "    # 这会将过渡的batch数组转换为batch数组的过渡\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # 计算非最终状态的掩码并连接batch元素\n",
    "    # （最终状态将是模拟结束后的状态）\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # 计算Q(s_t, a) - 模型计算Q(s_t)，然后我们选择所采取的动作列。\n",
    "    # 这些是根据policy_net对每个batch状态采取的操作\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # 计算所有下一个状态的V(s_{t+1})\n",
    "    # non_final_next_states的操作的预测值是基于“较旧的”target_net计算的\n",
    "    # 用max(1)[0]选择最佳奖励。这是基于掩码合并的，这样我们就可以得到预期的状态值，或者在状态是最终的情况下为0。\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # 计算预测的Q值\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # 计算Huber损失\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # 优化模型\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "fcd3d50b-84f5-46d0-a65b-cb7660ef6cf8"
   },
   "source": [
    "- 下面，你可以找到主要的训练循环。在开始时，我们重置环境并初始`state`张量。然后，我们采样一个动作并执行它，观察下一个屏幕和奖励（总是1），并优化我们的模型一次。当episode结束时（我们的模型失败），我们重新开始循环。\n",
    "- 下面，`num_episodes`设置为小数值。你应该下载笔记本并运行更多的episodes，例如300+以进行有意义的持续时间改进。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "f1c822a0-e9b9-4f84-9e8b-76e34a0063c3"
   },
   "outputs": [],
   "source": [
    "num_episodes = 50\n",
    "for i_episode in range(num_episodes):\n",
    "    # 初始化环境和状态\n",
    "    env.reset()\n",
    "    last_screen = get_screen()\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    for t in count():\n",
    "        # 选择动作并执行\n",
    "        action = select_action(state)\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # 观察新的状态\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen()\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # 在记忆中存储过滤\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # 移动到下一个状态\n",
    "        state = next_state\n",
    "\n",
    "        # 执行优化的一个步骤（在目标网络上）\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "    # 更新目标网络，复制DQN中的所有权重和偏差\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "tianchi_metadata": {
   "competitions": [],
   "datasets": [],
   "description": "",
   "notebookId": "163998",
   "source": "dsw"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
