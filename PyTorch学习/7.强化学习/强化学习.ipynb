{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习基本原理\n",
    "## Part1 什么是强化学习\n",
    "\n",
    "- 强化学习（英语：`Reinforcement learning`，简称`RL`）是机器学习中的一个领域，强调如何基于环境而行动，以取得最大化的预期利益。\n",
    "- 核心思想：智能体`agent`在环境`environment`中学习，根据环境的状态`state`（或观测到的`observation`），执行动作`action`，并根据环境的反馈 `reward`（奖励）来指导更好的动作。\n",
    "\n",
    "- 注意：从环境中获取的状态，有时候叫`state`，有时候叫`observation`，这两个其实一个代表全局状态，一个代表局部观测值，在多智能体环境里会有差别，但我们刚开始学习遇到的环境还没有那么复杂，可以先把这两个概念划上等号。\n",
    "\n",
    "<img src=\"https://cdn.nlark.com/yuque/0/2020/png/1508544/1592298601954-b94d291f-c24f-42ca-9008-058075a9f3b1.png\"/>\n",
    "\n",
    "## Part2 强化学习能做什么\n",
    "- 游戏（马里奥、Atari、Alpha Go、星际争霸等）\n",
    "- 机器人控制（机械臂、机器人、自动驾驶、四轴飞行器等）\n",
    "- 用户交互（推荐、广告、NLP等）\n",
    "- 交通（拥堵管理等）\n",
    "- 资源调度（物流、带宽、功率等）\n",
    "- 金融（投资组合、股票买卖等）\n",
    "- 其他\n",
    "\n",
    "## Part3 强化学习与监督学习的区别\n",
    "- 强化学习、监督学习、非监督学习是机器学习里的三个不同的领域，都跟深度学习有交集。\n",
    "- 监督学习寻找输入到输出之间的映射，比如分类和回归问题。\n",
    "- 非监督学习主要寻找数据之间的隐藏关系，比如聚类问题。\n",
    "- 强化学习则需要在与环境的交互中学习和寻找最佳决策方案。\n",
    "- 监督学习处理认知问题，强化学习处理决策问题。\n",
    "\n",
    "## Part4 强化学习的如何解决问题\n",
    "- 强化学习通过不断的试错探索，吸取经验和教训，持续不断的优化策略，从环境中拿到更好的反馈。\n",
    "- 强化学习有两种学习方案：基于价值(`value-based`)、基于策略(`policy-based`)\n",
    "\n",
    "## Part5 强化学习的算法和环境\n",
    "- 经典算法：`Q-learning`、`Sarsa`、`DQN`、`Policy Gradient`、`A3C`、`DDPG`、`PPO`\n",
    "- 环境分类：离散控制场景（输出动作可数）、连续控制场景（输出动作值不可数）\n",
    "- 强化学习经典环境库`GYM`将环境交互接口规范化为：重置环境`reset()`、交互`step()`、渲染`render()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN\n",
    "## 1. DQN简介\n",
    "- 表格型方法存储的状态数量有限，当面对围棋或机器人控制这类有数不清的状态的环境时，表格型方法在存储和查找效率上都受局限，`DQN`的提出解决了这一局限，使用神经网络来近似替代Q表格。\n",
    "- 本质上`DQN`还是一个`Q-learning`算法，更新方式一致。为了更好地探索环境，同样的也采用`ε-greedy`方法训练。\n",
    "- 在`Q-learning`的基础上，`DQN`提出了两个技巧使得`Q`网络的更新迭代更稳定。\n",
    "    - 经验回放`Experience Replay`：主要解决样本关联性和利用效率的问题。使用一个经验池存储多条经验`s,a,r,s'`，再从中随机抽取一批数据送去训练。\n",
    "    - 固定Q目标`Fixed-Q-Target`：主要解决算法训练不稳定的问题。复制一个和原来Q网络结构一样的`Target Q`网络，用于计算`Q`目标值。\n",
    "\n",
    "## 2. DQN算法\n",
    "- 我们的目标是训练处一种政策，试图最大化折现累积奖励$R_{t_0}=\\sum_{t=t_0}^{∞}\\gamma^{t-t_0}r_t$，其中$R_{t_0}$也称为回报。折扣应该是介于0和1之间的常数，以确保总和收敛。对于我们的代理来说，对比不确定的远期未来，它更看重它们相当有信心的不久的将来。\n",
    "- Q-learning背后的主要思想是，如果我们有一个函数$Q^*:State\\times Action\\to R$，它可以告诉我们的回报是什么，如果我们要在给定状态下采取行动，那么我们可以轻松地构建最大化我们奖励的政策：\n",
    "    - $\\pi^*(s)=\\arg\\max_aQ^*(s,a)$\n",
    "- 但是，我们不了解世界的一切，因此我们无法访问$Q^*$。但是，由于神经网络是通用函数逼近器，我们可以简单地创建一个并训练从而使得它类似于$Q^*$。\n",
    "- 对于我们的训练更新规则，我们将使用一个事实。即某些策略的每个Q函数都服从Bellman方程：\n",
    "    - $Q^{\\pi}(s,a)=r+\\gamma Q^{\\pi}(s',\\pi(s'))$\n",
    "- 平等的两边之间的差异被称为时间差异误差$\\delta$：\n",
    "    - $\\delta=Q(s,a)-(r+\\gamma\\max_aQ(s',a))$\n",
    "- 为了最大限度地降低此错误，我们将使用Huber损失。当误差很小时，Huber损失就像均方误差一样，但是当误差很大时，就像平均绝对误差一样。 当Q的估计噪声很多时，这使得它对异常值更加鲁棒。我们通过从重放内存中采样的一批转换B来计算：\n",
    "    - $L = \\frac{1}{|B|}\\sum_{(s,a,s',r)\\in B}L(\\delta)$\n",
    "    - $where L(\\delta)=\n",
    "    \\begin{cases}\n",
    "    \\frac{1}{2}\\delta^2, & for |\\delta| \\leq1,\\\\\n",
    "    |\\delta|-\\frac{1}{2}, & otherwise.\n",
    "    \\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 练习题\n",
    "- 如何为Q网络提供有标签的样本？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "tianchi_metadata": {
   "competitions": [],
   "datasets": [],
   "description": "",
   "notebookId": "162671",
   "source": "dsw"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
