{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.nlark.com/yuque/0/2020/png/1508544/1608275719214-edfbc5c2-063a-4f71-adc2-67290ca8f417.png\"/>\n",
    "\n",
    "SBERT论文：[Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084)  \n",
    "Pytorch版本Sentence-BERT仓库：https://github.com/UKPLab/sentence-transformers  \n",
    "- Sentence-BERT 在BERT的最后一层上添加了一个池化操作，并经过微调，可以推导出固定大小的sentence embedding。\n",
    "- SBERT 进一步使用Siamese和triplet神经网络更新模型的权重参数。\n",
    "- SBERT 使用Siamese孪生网络生成包含丰富语义的sentence embeddings，用于比较cosine-similarity。\n",
    "\n",
    "<img src=\"https://cdn.nlark.com/yuque/0/2020/png/1508544/1606268569362-33d8eac5-17b0-4df6-b891-d4d07851642c.png\"/>\n",
    "\n",
    "- $CosineSim(e_1,e_2)=\\frac{e_1\\cdot e_2}{\\lVert e_1 \\rVert \\cdot \\lVert e_2 \\rVert}$\n",
    "\n",
    "- SBERT采用计算两段文本向量的余弦相似度来完成预测（与模型训练过程不同），大幅缩短了响应时长。\n",
    "- 例如，SBERT论文中举例：从10000个句子中找到最相似的一对问句，因可能的组合太多（$10000 * (10000-1)/2=49995000$次计算），直接使用BERT计算需要65小时，而SBERT完成推理只需要5秒。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码成功从天池实验室点击编辑按钮加载到DSW，加载好的代码会⾃动打开，默认在<b>download</b>\n",
    "⽬录下<br>\n",
    "1、点击左侧的【天池】按钮<br>\n",
    "2、会出现【保存到天池】按钮和【添加数据源】模块，搜索Pytorch5.4，点击数据集中的下载按钮即可<br>\n",
    "###### （具体如下图所示）\n",
    "<center><img\n",
    "src=\"https://img.alicdn.com/imgextra/i4/O1CN01zsetgx1zaOBQbSDLs_!!6000000006730-2-\n",
    "tps-616-589.png\" width=60%></center>\n",
    "核⼼问题2\n",
    "数据集下载成功后，⻚⾯右上⻆会提示数据集下载成功，也会说名数据集存储位置，默认在\n",
    "<b>download</b>⽬录下，如下图所示。\n",
    "<center>\n",
    "<img\n",
    "src=\"https://img.alicdn.com/imgextra/i3/O1CN01uJzjgf1MLwg6jK7za_!!6000000001419-2-tps-\n",
    "1409-377.png\" width=60%>\n",
    "<img\n",
    "src=\"https://img.alicdn.com/imgextra/i1/O1CN01XQmAP027k1R811xls_!!6000000007834-2-\n",
    "tps-857-465.png\" width=60%>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util, models\n",
    "import torch\n",
    "from torch import nn\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载预训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对于句子/文本嵌入，我们希望将可变长度的输入文本映射到固定大小的Dense Vector。\n",
    "- 我们可以使用预先训练的SentenceTransformer模型，或者利用transformers上的model来从头开始创建网络体系结构。\n",
    "- 我们可以使用的最基本的网络体系结构如下：\n",
    "\n",
    "<img src=\"https://cdn.nlark.com/yuque/0/2020/png/1508544/1606268067356-f5d1ec76-0ac2-4fac-8106-3318fccd3a13.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载transformers上预训练模型 #\n",
    "model_name = './weights/chinese_roberta_wwm_ext'\n",
    "word_embedding_model = models.Transformer(model_name, max_seq_length=256)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(),\n",
    "                           out_features=256, activation_function=nn.Tanh())\n",
    "\n",
    "embedder = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])\n",
    "# 保证每次加载的embedding一致\n",
    "if not os.path.exists('model.pth'):\n",
    "    # 保存模型参数\n",
    "    torch.save(embedder.state_dict(), 'model.pth')\n",
    "else:\n",
    "    # 加载模型参数\n",
    "    embedder.load_state_dict(torch.load('model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 存储embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储语料embedding，方便快速取用 #\n",
    "batch_size = 128\n",
    "max_corpus_size = 307128  # 句子数量\n",
    "dataset_path = \"./totalnews.csv\" \n",
    "embedding_cache_path = 'quora-embeddings-{}-size-{}.pkl'.format(model_name.replace('/', '_'), max_corpus_size)  # 词向量存储地址\n",
    "# 检查 embedding cache path 是否存在\n",
    "if not os.path.exists(embedding_cache_path):\n",
    "    # 将csv文件中的数据转化成list\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    corpus_sentences = list(df['title'].map(lambda x: str(x)))\n",
    "    if len(corpus_sentences) >= max_corpus_size:\n",
    "        corpus_sentences = corpus_sentences[:max_corpus_size]\n",
    "    print(\"Encode the corpus. This might take a while\")\n",
    "    corpus_embeddings = embedder.encode(corpus_sentences, batch_size=batch_size,\n",
    "                                        show_progress_bar=True, convert_to_tensor=True)\n",
    "\n",
    "    print(\"Store file on disc\")\n",
    "    with open(embedding_cache_path, \"wb\") as fOut:\n",
    "        pickle.dump({'sentences': corpus_sentences, 'embeddings': corpus_embeddings}, fOut)\n",
    "else:\n",
    "    print(\"Load pre-computed embeddings from disc\")\n",
    "    with open(embedding_cache_path, \"rb\") as fIn:\n",
    "        cache_data = pickle.load(fIn)\n",
    "        corpus_sentences = cache_data['sentences']\n",
    "        corpus_embeddings = cache_data['embeddings']\n",
    "\n",
    "print(\"Corpus loaded with {} sentences / embeddings\".format(len(corpus_sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 执行主函数\n",
    "- 对于小型语料库（最多约10万个条目），我们可以计算查询与语料库中所有条目之间的余弦相似度。\n",
    "- 对于大型语料库，对所有分数进行排序将花费太多时间。因此，我们使用util.semantic_search函数仅获取前k个条目。\n",
    "    - sentence_transformers.util.semantic_search(query_embeddings: torch.Tensor, corpus_embeddings: torch.Tensor, query_chunk_size: int=100, corpus_chunk_size: int=100000, top_k: int=10)\n",
    "        - 这个函数在一堆 query embeddings 和 corpus embeddings 内执行余弦相似度搜索。\n",
    "        - 它可用于最多可达100万条的信息检索/语义搜索的语料库。\n",
    "        - 参数：\n",
    "            - query_embeddings - query embeddings的二维张量\n",
    "            - corpus_embeddings - corpus embeddings的二维张量\n",
    "            - query_chunk_size - 同时处理100个查询。增加这个值可以提高速度，但是需要更多的内存。\n",
    "            - corpus_chunk_size - 一次扫描10万条语料。增加这个值可以提高速度，但是需要更多的内存。\n",
    "            - top_k - 检索顶部k个匹配语料。注意：如果你的语料长度大于query_chunk_size，会返回。\n",
    "        - 返回：\n",
    "            - 返回一个按余弦相似度得分递减的排序列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_question = '京津冀秋冬'\n",
    "\n",
    "start_time = time.time()\n",
    "question_embedding = embedder.encode(inp_question, batch_size=batch_size, convert_to_tensor=True)\n",
    "hits = util.semantic_search(question_embedding, corpus_embeddings,\n",
    "                            query_chunk_size=1, corpus_chunk_size=len(corpus_sentences),\n",
    "                            top_k=10)\n",
    "end_time = time.time()  # 0.16s\n",
    "hits = hits[0]  # 得到第一个问题的hits\n",
    "\n",
    "print(\"Input question: \", inp_question)\n",
    "print(\"Results (after {:.3f} seconds):\".format(end_time-start_time))\n",
    "for hit in hits[0:5]:\n",
    "    print(\"\\t{:.3f}\\t{}\".format(hit['score'], corpus_sentences[hit['corpus_id']]))\n",
    "\n",
    "print(\"\\n\\n========\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "tianchi_metadata": {
   "competitions": [],
   "datasets": [],
   "description": "",
   "notebookId": "164173",
   "source": "dsw"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
