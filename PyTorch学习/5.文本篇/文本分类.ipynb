{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 零基础入门NLP-[新闻文本分类](https://tianchi.aliyun.com/competition/entrance/531810/information)\n",
    "- 文本分类的任务是将给定的文本划分到事先规定的文本类别。\n",
    "- 解题思路：\n",
    "    - 思路1：TF-IDF提取特征 + SVM分类\n",
    "    - 思路2：训练FastText词向量并分类\n",
    "    - 思路3：训练Word2Vec词向量 + TextCNN模型分类\n",
    "    - 思路4：训练BERT词向量并分类\n",
    "    - 思路5：BERT分类 + 统计特征的树模型\n",
    "\n",
    "<img src=\"https://cdn.nlark.com/yuque/0/2021/png/1508544/1614251188699-25bf14fd-9602-4db6-914b-aef600981658.png\"/>\n",
    "\n",
    "1. 赛题中每个新闻包含的字符个数平均为1000个，还有一些新闻字符较长；\n",
    "2. 赛题中新闻类别分布不均衡，科技类新闻样本量接近4万，星座类新闻(13)样本量不到1千；\n",
    "3. 赛题总共包括7000-8000个字符。\n",
    "\n",
    "# 一、赛题数据\n",
    "- 赛题以新闻数据为赛题数据，数据集报名后可见并可下载。赛题数据为新闻文本，并按照字符级别进行匿名处理。整合划分出14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐的文本数据。\n",
    "- 赛题数据由以下几个部分构成：训练集20w条样本，测试集A包括5w条样本，测试集B包括5w条样本。为了预防选手人工标注测试集的情况，我们将比赛数据的文本按照字符级别进行了匿名处理。处理后的赛题训练数据如下：\n",
    "\n",
    "|label|text|\n",
    "|---|---|\n",
    "|6|57 44 66 56 2 3 3 37 5 41 9 57 44 47 45 33 13 63 58 31 17 47 0 1 1 69 26 60 62 15 21 12 49 18 38 20 50 23 57 44 45 33 25 28 47 22 52 35 30 14 24 69 54 7 48 19 11 51 16 43 26 34 53 27 64 8 4 42 36 46 65 69 29 39 15 37 57 44 45 33 69 54 7 25 40 35 30 66 56 47 55 69 61 10 60 42 36 46 65 37 5 41 32 67 6 59 47 0 1 1 68|\n",
    "\n",
    "- 在数据集中标签的对应的关系如下：\n",
    "`{'科技': 0, '股票': 1, '体育': 2, '娱乐': 3, '时政': 4, '社会': 5, '教育': 6, '财经': 7, '家居': 8, '游戏': 9, '房产': 10, '时尚': 11, '彩票': 12, '星座': 13}`\n",
    "- 赛题数据来源为互联网上的新闻，通过收集并匿名处理得到。因此选手可以自行进行数据分析，可以充分发挥自己的特长来完成各种特征工程，不限制使用任何外部数据和模型。\n",
    "- 数据列使用\\t进行分割，Pandas读取数据的代码如下：\n",
    "`train_df = pd.read_csv('../input/train_set.csv', sep='\\t')`\n",
    "\n",
    "# 二、评测标准\n",
    "- 评价标准为类别f1_score的均值，选手提交结果与实际测试集的类别进行对比，结果越大越好。\n",
    "    - 计算公式：$F1=2*\\frac{(precision * recall)}{(preci)}$\n",
    "- 可以通过sklearn完成f1_score计算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26666666666666666"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "y_true = [0, 1, 2, 0, 1, 2]\n",
    "y_pred = [0, 2, 1, 0, 0, 1]\n",
    "f1_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、机器学习\n",
    "## 3.1 导入相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC  # 可以使用其它机器学习模型\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 读入数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码成功从天池实验室点击编辑按钮加载到DSW，加载好的代码会⾃动打开，默认在<b>download</b>\n",
    "⽬录下<br>\n",
    "1、点击左侧的【天池】按钮<br>\n",
    "2、会出现【保存到天池】按钮和【添加数据源】模块，搜索新闻文本分类，点击数据集中的下载按钮即可<br>\n",
    "###### （具体如下图所示）\n",
    "<center><img\n",
    "src=\"https://img.alicdn.com/imgextra/i4/O1CN01zsetgx1zaOBQbSDLs_!!6000000006730-2-\n",
    "tps-616-589.png\" width=60%></center>\n",
    "核⼼问题2\n",
    "数据集下载成功后，⻚⾯右上⻆会提示数据集下载成功，也会说名数据集存储位置，默认在\n",
    "<b>download</b>⽬录下，如下图所示。\n",
    "<center>\n",
    "<img\n",
    "src=\"https://img.alicdn.com/imgextra/i3/O1CN01uJzjgf1MLwg6jK7za_!!6000000001419-2-tps-\n",
    "1409-377.png\" width=60%>\n",
    "<img\n",
    "src=\"https://img.alicdn.com/imgextra/i1/O1CN01XQmAP027k1R811xls_!!6000000007834-2-\n",
    "tps-857-465.png\" width=60%>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./train_set.csv', sep='\\t', nrows=8000)\n",
    "test_df = pd.read_csv('./test_a.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 文本表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1,1),\n",
    "    max_features=1000)\n",
    "\n",
    "tfidf.fit(pd.concat([train_df['text'], test_df['text']]))\n",
    "train_word_features = tfidf.transform(train_df['text'])\n",
    "test_word_features = tfidf.transform(test_df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 折交叉验证开始...\n",
      "LinearSVC准确率为： 0.8653196365489803\n",
      "第 2 折交叉验证开始...\n",
      "LinearSVC准确率为： 0.8635863117425737\n",
      "第 3 折交叉验证开始...\n",
      "LinearSVC准确率为： 0.8663983874833651\n",
      "第 4 折交叉验证开始...\n",
      "LinearSVC准确率为： 0.8810913779158512\n",
      "第 5 折交叉验证开始...\n",
      "LinearSVC准确率为： 0.8524364220474782\n"
     ]
    }
   ],
   "source": [
    "X_train = train_word_features\n",
    "y_train = train_df['label']\n",
    "X_test = test_word_features\n",
    "\n",
    "KF = KFold(n_splits=5, random_state=7) \n",
    "clf = LinearSVC()\n",
    "# 存储测试集预测结果 行数：len(X_test) ,列数：1列\n",
    "test_pred = np.zeros((X_test.shape[0], 1), int)  \n",
    "for KF_index, (train_index,valid_index) in enumerate(KF.split(X_train)):\n",
    "    print('第', KF_index+1, '折交叉验证开始...')\n",
    "    # 训练集划分\n",
    "    x_train_, x_valid_ = X_train[train_index], X_train[valid_index]\n",
    "    y_train_, y_valid_ = y_train[train_index], y_train[valid_index]\n",
    "    # 模型构建\n",
    "    clf.fit(x_train_, y_train_)\n",
    "    # 模型预测\n",
    "    val_pred = clf.predict(x_valid_)\n",
    "    print(\"LinearSVC准确率为：\",f1_score(y_valid_, val_pred, average='macro'))\n",
    "    # 保存测试集预测结果\n",
    "    test_pred = np.column_stack((test_pred, clf.predict(X_test)))  # 将矩阵按列合并\n",
    "# 多数投票\n",
    "preds = []\n",
    "for i, test_list in enumerate(test_pred):\n",
    "    preds.append(np.argmax(np.bincount(test_list)))\n",
    "preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 输出上传文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./test_a_sample_submit.csv')\n",
    "submission['label'] = preds\n",
    "submission.to_csv('./LinearSVC_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 存在问题\n",
    "- 虽然n元语法能够体现邻接词组的关系，但是它难以捕捉句子中距离较远的词和词之间的关系。\n",
    "- 每个新闻平均字符个数较多，可能需要截断。\n",
    "- 由于类别不均衡，会严重影响模型的精度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、FastText\n",
    "FastText在文本分类任务上是优于TF-IDF的：\n",
    "- FastText用单词的Embedding叠加获得的文档向量，将相似的句子分为一类；\n",
    "- FastText学习到的Embedding空间维度比较低，可以快速进行训练。\n",
    "\n",
    "## 4.1 导入相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fasttext\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 读入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./train_set.csv', sep='\\t', nrows=8000)\n",
    "test_df = pd.read_csv('./test_a.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 文本预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换为FastText需要的格式\n",
    "train_df['label_ft'] = '__label__' + train_df['label'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 训练模型\n",
    "设计可以评估各个类别的性能度量函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各个类别性能度量的函数\n",
    "def category_performance_measure(labels_right, labels_pred):\n",
    "    text_labels = list(set(labels_right))\n",
    "    text_pred_labels = list(set(labels_pred))\n",
    "    \n",
    "    TP = dict.fromkeys(text_labels,0)  #预测正确的各个类的数目\n",
    "    TP_FP = dict.fromkeys(text_labels,0)   #测试数据集中各个类的数目\n",
    "    TP_FN = dict.fromkeys(text_labels,0) #预测结果中各个类的数目\n",
    "    \n",
    "    # 计算TP等数量\n",
    "    for i in range(0,len(labels_right)):\n",
    "        TP_FP[labels_right[i]] += 1\n",
    "        TP_FN[labels_pred[i]] += 1\n",
    "        if labels_right[i] == labels_pred[i]:\n",
    "            TP[labels_right[i]] += 1\n",
    "    #计算准确率P，召回率R，F1值\n",
    "    for key in TP_FP:\n",
    "        P = float(TP[key]) / float(TP_FP[key] + 1)\n",
    "        R = float(TP[key]) / float(TP_FN[key] + 1)\n",
    "        F1 = P * R * 2 / (P + R) if (P + R) != 0 else 0\n",
    "        print(\"%s:\\t P:%f\\t R:%f\\t F1:%f\" % (key,P,R,F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastText是一种融合深度学习和机器学习各自优点的文本分类模型，速度非常快，但是模型结构简单，效果还算中上游。由于其使用词袋思想，语义信息获取有限。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 折交叉验证开始...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fasttext准确率为： 0.19447980178703447\n",
      "0:\t P:0.762987\t R:0.661972\t F1:0.708899\n",
      "1:\t P:0.942652\t R:0.659148\t F1:0.775811\n",
      "2:\t P:0.955556\t R:0.510891\t F1:0.665806\n",
      "3:\t P:0.802139\t R:0.434783\t F1:0.563910\n",
      "4:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "5:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "6:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "7:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "8:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "9:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "10:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "11:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "12:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "13:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "第 2 折交叉验证开始...\n",
      "Fasttext准确率为： 0.19857951330734983\n",
      "0:\t P:0.816129\t R:0.598109\t F1:0.690314\n",
      "1:\t P:0.909396\t R:0.705729\t F1:0.794721\n",
      "2:\t P:0.969466\t R:0.577273\t F1:0.723647\n",
      "3:\t P:0.810526\t R:0.431373\t F1:0.563071\n",
      "4:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "5:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "6:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "7:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "8:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "9:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "10:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "11:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "12:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "13:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "第 3 折交叉验证开始...\n",
      "Fasttext准确率为： 0.19179660880154578\n",
      "0:\t P:0.798077\t R:0.601449\t F1:0.685950\n",
      "1:\t P:0.868687\t R:0.761062\t F1:0.811321\n",
      "2:\t P:0.983051\t R:0.513274\t F1:0.674419\n",
      "3:\t P:0.822857\t R:0.364557\t F1:0.505263\n",
      "4:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "5:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "6:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "7:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "8:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "9:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "10:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "11:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "12:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "13:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "第 4 折交叉验证开始...\n",
      "Fasttext准确率为： 0.1990341809802814\n",
      "0:\t P:0.860068\t R:0.495088\t F1:0.628429\n",
      "1:\t P:0.901961\t R:0.743935\t F1:0.815362\n",
      "2:\t P:0.938224\t R:0.680672\t F1:0.788961\n",
      "3:\t P:0.878788\t R:0.395095\t F1:0.545113\n",
      "4:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "5:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "6:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "7:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "8:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "9:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "10:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "11:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "12:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "13:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "第 5 折交叉验证开始...\n",
      "Fasttext准确率为： 0.1955341934834531\n",
      "0:\t P:0.875000\t R:0.517510\t F1:0.650367\n",
      "1:\t P:0.852761\t R:0.778711\t F1:0.814056\n",
      "2:\t P:0.915612\t R:0.628986\t F1:0.745704\n",
      "3:\t P:0.862275\t R:0.371134\t F1:0.518919\n",
      "4:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "5:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "6:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "7:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "8:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "9:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "10:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "11:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "12:\t P:0.000000\t R:0.000000\t F1:0.000000\n",
      "13:\t P:0.000000\t R:0.000000\t F1:0.000000\n"
     ]
    }
   ],
   "source": [
    "X_train = train_df['text']\n",
    "y_train = train_df['label']\n",
    "X_test = test_df['text']\n",
    "KF = KFold(n_splits=5, random_state=7, shuffle=True)\n",
    "test_pred = np.zeros((X_test.shape[0], 1), int)  # 存储测试集预测结果 行数：len(X_test) ,列数：1列\n",
    "for KF_index, (train_index,valid_index) in enumerate(KF.split(X_train)):\n",
    "    print('第', KF_index+1, '折交叉验证开始...')\n",
    "    # 转换为FastText需要的格式\n",
    "    train_df[['text','label_ft']].iloc[train_index].to_csv('train_df.csv', header=None, index=False, sep='\\t')\n",
    "    # 模型构建\n",
    "    model = fasttext.train_supervised('train_df.csv', lr=0.1, epoch=27, wordNgrams=5, \n",
    "                                      verbose=2, minCount=1, loss='hs')\n",
    "    # 模型预测\n",
    "    val_pred = [int(model.predict(x)[0][0].split('__')[-1]) for x in X_train.iloc[valid_index]]\n",
    "    print('Fasttext准确率为：',f1_score(list(y_train.iloc[valid_index]), val_pred, average='macro'))\n",
    "    category_performance_measure(list(y_train.iloc[valid_index]), val_pred)\n",
    "    \n",
    "    # 保存测试集预测结果\n",
    "    test_pred_ = [int(model.predict(x)[0][0].split('__')[-1]) for x in X_test]\n",
    "    test_pred = np.column_stack((test_pred, test_pred_))  # 将矩阵按列合并\n",
    "# 取测试集中预测数量最多的数\n",
    "preds = []\n",
    "for i, test_list in enumerate(test_pred):\n",
    "    preds.append(np.argmax(np.bincount(test_list)))\n",
    "preds = np.array(preds) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 输出上传文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./test_a_sample_submit.csv')\n",
    "submission['label'] = preds\n",
    "submission.to_csv('./LinearSVC_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五、BERT\n",
    "\n",
    "<img src=\"https://cdn.nlark.com/yuque/0/2021/png/1508544/1614252341600-908e79ed-86c4-45b9-b661-9fd77d6c3681.png\"/>\n",
    "<img src=\"https://cdn.nlark.com/yuque/0/2021/png/1508544/1614252349214-4ca253ae-ce85-4655-8a44-63fd715df607.png\"/>\n",
    "\n",
    "## 5.1 导入相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 读入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./train_set.csv', sep='\\t')\n",
    "test_df = pd.read_csv('./test_a.csv', sep='\\t')\n",
    "test_df['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./emb/bert-mini/vocab.txt')\n",
    "tokenizer.encode_plus(\"2967 6758 339 2021 1854\",\n",
    "        add_special_tokens=True,\n",
    "        max_length=20,\n",
    "        truncation=True)\n",
    "# token_type_ids 通常第一个句子全部标记为0，第二个句子全部标记为1。\n",
    "# attention_mask padding的地方为0，未padding的地方为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.comment_text = self.data.text\n",
    "        self.targets = self.data.label\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comment_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        comment_text = self.comment_text[index]\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            comment_text,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "MAX_LEN = 256\n",
    "train_size = 0.8\n",
    "train_dataset = train_df.sample(frac=train_size,random_state=7)\n",
    "valid_dataset = train_df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(train_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"VALID Dataset: {}\".format(valid_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_df.shape))\n",
    "\n",
    "train_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
    "valid_set = CustomDataset(valid_dataset, tokenizer, MAX_LEN)\n",
    "test_set = CustomDataset(test_df, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 16\n",
    "TEST_BATCH_SIZE = 16\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True}\n",
    "\n",
    "valid_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True}\n",
    "\n",
    "test_params = {'batch_size': TEST_BATCH_SIZE,\n",
    "                'shuffle': False}\n",
    "\n",
    "train_loader = DataLoader(train_set, **train_params)\n",
    "valid_loader = DataLoader(valid_set, **valid_params)\n",
    "test_loader = DataLoader(test_set, **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 模型创建\n",
    "预训练BERT以及相关代码下载地址：  \n",
    "链接: https://pan.baidu.com/s/1zd6wN7elGgp1NyuzYKpvGQ 提取码: tmp5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.config = BertConfig.from_pretrained('./emb/bert-mini/bert_config.json', output_hidden_states=True)\n",
    "        self.l1 = BertModel.from_pretrained('./emb/bert-mini/pytorch_model.bin', config=self.config)\n",
    "        self.bilstm1 = torch.nn.LSTM(512, 64, 1, bidirectional=True)\n",
    "        self.l2 = torch.nn.Linear(128, 64)\n",
    "        self.a1 = torch.nn.ReLU()\n",
    "        self.l3 = torch.nn.Dropout(0.3)\n",
    "        self.l4 = torch.nn.Linear(64, 14)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        sequence_output, pooler_output, hidden_states= self.l1(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        # [bs, 200, 256]  [bs,256]\n",
    "        bs = len(sequence_output)\n",
    "        h12 = hidden_states[-1][:,0].view(1,bs,256)\n",
    "        h11 = hidden_states[-2][:,0].view(1,bs,256)\n",
    "        concat_hidden = torch.cat((h12,h11),2)\n",
    "        x, _ = self.bilstm1(concat_hidden)\n",
    "        x = self.l2(x.view(bs,128))\n",
    "        x = self.a1(x)\n",
    "        x = self.l3(x)\n",
    "        output = self.l4(x)\n",
    "        return output\n",
    "\n",
    "net = BERTClass()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数设置\n",
    "lr, num_epochs = 1e-5, 30\n",
    "criterion = torch.nn.CrossEntropyLoss()  # 选择损失函数\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)  # 选择优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, device=torch.device('cpu')):\n",
    "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
    "    acc_sum, n = torch.tensor([0], dtype=torch.float32,device=device), 0\n",
    "    y_pred_, y_true_ = [], []\n",
    "    for data in tqdm(data_iter):\n",
    "        # If device is the GPU, copy the data to the GPU.\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "        net.eval()\n",
    "        y_hat_ = net(ids, mask, token_type_ids)\n",
    "        with torch.no_grad():\n",
    "            targets = targets.long()\n",
    "            # [[0.2 ,0.4 ,0.5 ,0.6 ,0.8] ,[ 0.1,0.2 ,0.4 ,0.3 ,0.1]] => [ 4 , 2 ]\n",
    "            acc_sum += torch.sum((torch.argmax(y_hat_, dim=1) == targets))\n",
    "            y_pred_.extend(torch.argmax(y_hat_, dim=1).cpu().numpy().tolist())\n",
    "            y_true_.extend(targets.cpu().numpy().tolist())\n",
    "            n += targets.shape[0]\n",
    "    valid_f1 = metrics.f1_score(y_true_, y_pred_, average='macro')\n",
    "    return acc_sum.item()/n, valid_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch,train_iter, test_iter, criterion, num_epochs, optimizer, device):\n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "    best_test_f1 = 0\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # 设置学习率下降策略\n",
    "#     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5, eta_min=2e-06)  # 余弦退火\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum = torch.tensor([0.0], dtype=torch.float32, device=device)\n",
    "        train_acc_sum = torch.tensor([0.0], dtype=torch.float32, device=device)\n",
    "        n, start = 0, time.time()\n",
    "        y_pred, y_true = [], []\n",
    "        for data in tqdm(train_iter):\n",
    "            net.train()\n",
    "            optimizer.zero_grad()\n",
    "            ids = data['ids'].to(device, dtype=torch.long)\n",
    "            mask = data['mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            y_hat = net(ids, mask, token_type_ids)\n",
    "            loss = criterion(y_hat, targets.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                targets = targets.long()\n",
    "                train_l_sum += loss.float()\n",
    "                train_acc_sum += (torch.sum((torch.argmax(y_hat, dim=1) == targets))).float()\n",
    "                y_pred.extend(torch.argmax(y_hat, dim=1).cpu().numpy().tolist())\n",
    "                y_true.extend(targets.cpu().numpy().tolist())\n",
    "                n += targets.shape[0]\n",
    "        valid_acc, valid_f1 = evaluate_accuracy(test_iter, net, device)\n",
    "        train_acc = train_acc_sum / n\n",
    "        train_f1 = metrics.f1_score(y_true, y_pred, average='macro')\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, valid acc %.3f, '\n",
    "              'train f1 %.3f, valid f1 %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc, valid_acc,\n",
    "                 train_f1, valid_f1, time.time() - start))\n",
    "        if valid_f1 > best_test_f1:\n",
    "            print('find best! save at model/best.pth')\n",
    "            best_test_f1 = valid_f1\n",
    "            torch.save(net.state_dict(), 'model/best.pth')\n",
    "        scheduler.step()  # 更新学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(net,train_loader, valid_loader, criterion, num_epochs, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(net, test_iter):\n",
    "    # 预测模型\n",
    "    preds_list = []\n",
    "    print('加载最优模型')\n",
    "    net.load_state_dict(torch.load('model/best.pth'))\n",
    "    net = net.to(device)\n",
    "    print('inference测试集')\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_iter):\n",
    "            ids = data['ids'].to(device, dtype=torch.long)\n",
    "            mask = data['mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            batch_preds = list(net(ids, mask, token_type_ids).argmax(dim=1).cpu().numpy())\n",
    "            for preds in batch_preds:\n",
    "                preds_list.append(preds)           \n",
    "    return preds_list\n",
    "preds_list = model_predict(net, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./test_a_sample_submit.csv')\n",
    "submission['label'] = preds_list\n",
    "submission.to_csv('./submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 练习题\n",
    "对代码进行修改，分别计算验证集每个类的准确率、召回率和F1 score。  \n",
    "\n",
    "<img src=\"https://cdn.nlark.com/yuque/0/2021/png/1508544/1614252224981-4d1f5ca6-4cd7-4885-bb83-aa4180774a21.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "tianchi_metadata": {
   "competitions": [],
   "datasets": [
    {
     "id": "92372",
     "title": "新闻文本分类"
    }
   ],
   "description": "",
   "notebookId": "163425",
   "source": "dsw"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
