{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 零基础入门CV——[街景字符编码识别](https://tianchi.aliyun.com/competition/entrance/531795/forum)\n",
    "# 1. 赛题数据\n",
    "- 赛题来源自Google街景图像中的门牌号数据集（The Street View House Numbers Dataset, SVHN），并根据一定方式采样得到比赛数据集。\n",
    "- 数据集报名后可见并可下载，该数据来自真实场景的门牌号。训练集数据包括3W张照片，验证集数据包括1W张照片，每张照片包括颜色图像和对应的编码类别和具体位置；为了保证比赛的公平性，测试集A包括4W张照片，测试集B包括4W张照片。\n",
    "\n",
    "<img src=\"https://cdn.nlark.com/yuque/0/2021/jpeg/1508544/1614237803853-a4d5d4fb-9fd3-4fc9-acea-e50a80d6d359.jpeg\"/>\n",
    "\n",
    "- 需要注意的是本赛题需要选手识别图片中所有的字符，为了降低比赛难度，我们提供了训练集、验证集和测试集中字符的位置框。\n",
    "\n",
    "## 字段表\n",
    "- 所有的数据（训练集、验证集和测试集）的标注使用JSON格式，并使用文件名进行索引。如果一个文件中包括多个字符，则使用列表将字段进行组合。\n",
    "\n",
    "|Field|Description|\n",
    "|---|---|\n",
    "|top|左上角坐标Y|\n",
    "|height|字符高度|\n",
    "|left|左上角坐标X|\n",
    "|width|字符宽度|\n",
    "|label|字符编码|\n",
    "\n",
    "- 注：数据集来源自SVHN，[网页链接](http://ufldl.stanford.edu/housenumbers/)，并进行匿名处理和噪音处理，请各位选手使用比赛给定的数据集完成训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 评测标准\n",
    "- 评价标准为准确率，选手提交结果与实际图片的编码进行对比，以编码整体识别准确率为评价指标，结果越大越好，具体计算公式如下：\n",
    "    - $score=\\frac{编码识别正确的数量}{测试集图片数量}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 实现\n",
    "## 3.1 构建数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: opencv-python in /data/nas/workspace/envs/python3.6/site-packages (4.5.2.52)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.6/site-packages (from opencv-python) (1.19.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: click in /data/nas/workspace/envs/python3.6/site-packages (8.0.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from click) (3.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->click) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /data/nas/workspace/envs/python3.6/site-packages (from importlib-metadata->click) (3.10.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: editdistance in /data/nas/workspace/envs/python3.6/site-packages (0.5.3)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 需要提前安装cv2、click、editdistance，加⼊下⽅代码\n",
    "!pip install opencv-python --user\n",
    "!pip install click --user\n",
    "!pip install editdistance --user\n",
    "# pip按装包后，如果导⼊过程还出错，建议尝试重启kernel或刷新⻚⾯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码成功从天池实验室点击编辑按钮加载到DSW，加载好的代码会⾃动打开，默认在<b>download</b>\n",
    "⽬录下<br>\n",
    "1、点击左侧的【天池】按钮<br>\n",
    "2、会出现【保存到天池】按钮和【添加数据源】模块，搜索Pytorch_mchar_data_list，点击数据集中的下载按钮即可<br>\n",
    "###### （具体如下图所示）\n",
    "<center><img\n",
    "src=\"https://img.alicdn.com/imgextra/i4/O1CN01zsetgx1zaOBQbSDLs_!!6000000006730-2-\n",
    "tps-616-589.png\" width=60%></center>\n",
    "核⼼问题2\n",
    "数据集下载成功后，⻚⾯右上⻆会提示数据集下载成功，也会说名数据集存储位置，默认在\n",
    "<b>download</b>⽬录下，如下图所示。\n",
    "<center>\n",
    "<img\n",
    "src=\"https://img.alicdn.com/imgextra/i3/O1CN01uJzjgf1MLwg6jK7za_!!6000000001419-2-tps-\n",
    "1409-377.png\" width=60%>\n",
    "<img\n",
    "src=\"https://img.alicdn.com/imgextra/i1/O1CN01XQmAP027k1R811xls_!!6000000007834-2-\n",
    "tps-857-465.png\" width=60%>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os, json\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def text_collate(batch):\n",
    "    img = list()\n",
    "    seq = list()\n",
    "    seq_len = list()\n",
    "    for sample in batch:\n",
    "        img.append(torch.from_numpy(sample[\"img\"].transpose((2, 0, 1))).float())\n",
    "        seq.extend(sample[\"seq\"])\n",
    "        seq_len.append(sample[\"seq_len\"])\n",
    "    img = torch.stack(img)\n",
    "    seq = torch.Tensor(seq).int()\n",
    "    seq_len = torch.Tensor(seq_len).int()\n",
    "    batch = {\"img\": img, \"seq\": seq, \"seq_len\": seq_len}\n",
    "    return batch\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        sample[\"img\"] = torch.from_numpy(sample[\"img\"].transpose((2, 0, 1))).float()\n",
    "        sample[\"seq\"] = torch.Tensor(sample[\"seq\"]).int()\n",
    "        return sample\n",
    "\n",
    "\n",
    "class Resize(object):\n",
    "    def __init__(self, size=(320, 32)):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        sample[\"img\"] = cv2.resize(sample[\"img\"], self.size)\n",
    "        return sample\n",
    "\n",
    "\n",
    "class Rotation(object):\n",
    "    def __init__(self, angle=5, fill_value=0, p = 0.5):\n",
    "        self.angle = angle\n",
    "        self.fill_value = fill_value\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if np.random.uniform(0.0, 1.0) < self.p or not sample[\"aug\"]:\n",
    "            return sample\n",
    "        h,w,_ = sample[\"img\"].shape\n",
    "        ang_rot = np.random.uniform(self.angle) - self.angle/2\n",
    "        transform = cv2.getRotationMatrix2D((w/2, h/2), ang_rot, 1)\n",
    "        sample[\"img\"] = cv2.warpAffine(sample[\"img\"], transform, (w,h), borderValue = self.fill_value)\n",
    "        return sample\n",
    "\n",
    "\n",
    "class Translation(object):\n",
    "    def __init__(self, fill_value=0, p = 0.5):\n",
    "        self.fill_value = fill_value\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if np.random.uniform(0.0, 1.0) < self.p or not sample[\"aug\"]:\n",
    "            return sample\n",
    "        h,w,_ = sample[\"img\"].shape\n",
    "        trans_range = [w / 10, h / 10]\n",
    "        tr_x = trans_range[0]*np.random.uniform()-trans_range[0]/2\n",
    "        tr_y = trans_range[1]*np.random.uniform()-trans_range[1]/2\n",
    "        transform = np.float32([[1,0, tr_x], [0,1, tr_y]])\n",
    "        sample[\"img\"] = cv2.warpAffine(sample[\"img\"], transform, (w,h), borderValue = self.fill_value)\n",
    "        return sample\n",
    "\n",
    "\n",
    "class Scale(object):\n",
    "    def __init__(self, scale=[0.5, 1.2], fill_value=0, p = 0.5):\n",
    "        self.scale = scale\n",
    "        self.fill_value = fill_value\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if np.random.uniform(0.0, 1.0) < self.p or not sample[\"aug\"]:\n",
    "            return sample\n",
    "        h, w, _ = sample[\"img\"].shape\n",
    "        scale = np.random.uniform(self.scale[0], self.scale[1])\n",
    "        transform = np.float32([[scale, 0, 0],[0, scale, 0]])\n",
    "        sample[\"img\"] = cv2.warpAffine(sample[\"img\"], transform, (w,h), borderValue = self.fill_value)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_path, data_label, transform=None):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.data_label = data_label\n",
    "        self.transform = transform\n",
    "\n",
    "    def abc_len(self):\n",
    "        return len('0123456789')\n",
    "\n",
    "    def get_abc(self):\n",
    "        return '0123456789'\n",
    "\n",
    "    def set_mode(self, mode):\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data_label[idx]\n",
    "\n",
    "        img = cv2.imread(self.data_path[idx])\n",
    "        seq = self.text_to_seq(text)\n",
    "        sample = {\"img\": img, \"seq\": seq, \"seq_len\": len(seq), \"aug\": 1}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "    def text_to_seq(self, text):\n",
    "        seq = []\n",
    "        for c in text:\n",
    "            seq.append(self.get_abc().find(str(c)) + 1)\n",
    "        return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 构建CRNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision.models as models\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 abc='0123456789',\n",
    "                 backend='resnet18',\n",
    "                 rnn_hidden_size=64,\n",
    "                 rnn_num_layers=1,\n",
    "                 rnn_dropout=0,\n",
    "                 seq_proj=[0, 0]):\n",
    "        super(CRNN, self).__init__()\n",
    "\n",
    "        self.abc = abc\n",
    "        self.num_classes = len(self.abc)\n",
    "\n",
    "        self.feature_extractor = getattr(models, backend)(pretrained=True)\n",
    "        self.cnn = nn.Sequential(\n",
    "            self.feature_extractor.conv1,\n",
    "            self.feature_extractor.bn1,\n",
    "            self.feature_extractor.relu,\n",
    "            self.feature_extractor.maxpool,\n",
    "            self.feature_extractor.layer1,\n",
    "            self.feature_extractor.layer2,\n",
    "            self.feature_extractor.layer3,\n",
    "            self.feature_extractor.layer4\n",
    "        )\n",
    "\n",
    "        self.fully_conv = seq_proj[0] == 0\n",
    "        if not self.fully_conv:\n",
    "            self.proj = nn.Conv2d(seq_proj[0], seq_proj[1], kernel_size=1)\n",
    "\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn_num_layers = rnn_num_layers\n",
    "        self.rnn = nn.GRU(self.get_block_size(self.cnn),\n",
    "                          rnn_hidden_size, rnn_num_layers,\n",
    "                          batch_first=False,\n",
    "                          dropout=rnn_dropout, bidirectional=True)\n",
    "        self.linear = nn.Linear(rnn_hidden_size * 2, self.num_classes + 1)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x, decode=False):\n",
    "        hidden = self.init_hidden(x.size(0), next(self.parameters()).is_cuda)\n",
    "        features = self.cnn(x)\n",
    "        features = self.features_to_sequence(features)\n",
    "        seq, hidden = self.rnn(features, hidden)\n",
    "        seq = self.linear(seq)\n",
    "        if not self.training:\n",
    "            seq = self.softmax(seq)\n",
    "            if decode:\n",
    "                seq = self.decode(seq)\n",
    "        return seq\n",
    "\n",
    "    def init_hidden(self, batch_size, gpu=False):\n",
    "        h0 = Variable(torch.zeros( self.rnn_num_layers * 2,\n",
    "                                   batch_size,\n",
    "                                   self.rnn_hidden_size))\n",
    "        if gpu:\n",
    "            h0 = h0.cuda()\n",
    "        return h0\n",
    "\n",
    "    def features_to_sequence(self, features):\n",
    "        features = features.mean(2)\n",
    "        b, c, w = features.size()\n",
    "        features = features.reshape(b, c, 1, w)\n",
    "        b, c, h, w = features.size()\n",
    "        # print(b, c, h, w)\n",
    "        assert h == 1, \"the height of out must be 1\"\n",
    "        if not self.fully_conv:\n",
    "            features = features.permute(0, 3, 2, 1)\n",
    "            features = self.proj(features)\n",
    "            features = features.permute(1, 0, 2, 3)\n",
    "        else:\n",
    "            features = features.permute(3, 0, 2, 1)\n",
    "        features = features.squeeze(2)\n",
    "        return features\n",
    "\n",
    "    def get_block_size(self, layer):\n",
    "        return layer[-1][-1].bn2.weight.size()[0]\n",
    "\n",
    "    def pred_to_string(self, pred):\n",
    "        seq = []\n",
    "        for i in range(pred.shape[0]):\n",
    "            label = np.argmax(pred[i])\n",
    "            seq.append(label - 1)\n",
    "        out = []\n",
    "        for i in range(len(seq)):\n",
    "            if len(out) == 0:\n",
    "                if seq[i] != -1:\n",
    "                    out.append(seq[i])\n",
    "            else:\n",
    "                if seq[i] != -1 and seq[i] != seq[i - 1]:\n",
    "                    out.append(seq[i])\n",
    "        out = ''.join(self.abc[i] for i in out)\n",
    "        return out\n",
    "\n",
    "    def decode(self, pred):\n",
    "        pred = pred.permute(1, 0, 2).cpu().data.numpy()\n",
    "        seq = []\n",
    "        for i in range(pred.shape[0]):\n",
    "            seq.append(self.pred_to_string(pred[i]))\n",
    "        return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def load_weights(target, source_state):\n",
    "    new_dict = OrderedDict()\n",
    "    for k, v in target.state_dict().items():\n",
    "        if k in source_state and v.size() == source_state[k].size():\n",
    "            new_dict[k] = source_state[k]\n",
    "        else:\n",
    "            new_dict[k] = v\n",
    "    target.load_state_dict(new_dict)\n",
    "\n",
    "def load_model(abc, seq_proj=[0, 0], backend='resnet18', snapshot=None, cuda=True):\n",
    "    net = CRNN(abc=abc, seq_proj=seq_proj, backend=backend)\n",
    "    # net = nn.DataParallel(net)\n",
    "    if snapshot is not None:\n",
    "        load_weights(net, torch.load(snapshot))\n",
    "    if cuda:\n",
    "        net = net.cuda()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepLR(object):\n",
    "    def __init__(self, optimizer, step_size=1000, max_iter=10000):\n",
    "        self.optimizer = optimizer\n",
    "        self.max_iter = max_iter\n",
    "        self.step_size = step_size\n",
    "        self.last_iter = -1\n",
    "        self.base_lrs = list(map(lambda group: group['lr'], optimizer.param_groups))\n",
    "\n",
    "    def get_lr(self):\n",
    "        return self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "    def step(self, last_iter=None):\n",
    "        if last_iter is not None:\n",
    "            self.last_iter = last_iter\n",
    "        if self.last_iter + 1 == self.max_iter:\n",
    "            self.last_iter = -1\n",
    "        self.last_iter = (self.last_iter + 1) % self.max_iter\n",
    "        for ids, param_group in enumerate(self.optimizer.param_groups):\n",
    "            param_group['lr'] = self.base_lrs[ids] * 0.8 ** ( self.last_iter // self.step_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import string\n",
    "from tqdm import tqdm_notebook\n",
    "import click\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "import editdistance\n",
    "\n",
    "def test(net, data, abc, cuda, batch_size=50):\n",
    "    data_loader = DataLoader(data, batch_size=batch_size, num_workers=4, shuffle=False, collate_fn=text_collate)\n",
    "\n",
    "    count = 0\n",
    "    tp = 0\n",
    "    avg_ed = 0\n",
    "    iterator = tqdm_notebook(data_loader)\n",
    "    for sample in iterator:\n",
    "        imgs = Variable(sample[\"img\"])\n",
    "        if cuda:\n",
    "            imgs = imgs.cuda()\n",
    "        out = net(imgs, decode=True)\n",
    "        gt = (sample[\"seq\"].numpy() - 1).tolist()\n",
    "        lens = sample[\"seq_len\"].numpy().tolist()\n",
    "        pos = 0\n",
    "        key = ''\n",
    "        for i in range(len(out)):\n",
    "            gts = ''.join(abc[c] for c in gt[pos:pos+lens[i]])\n",
    "            pos += lens[i]\n",
    "            if gts == out[i]:\n",
    "                tp += 1\n",
    "            else:\n",
    "                avg_ed += editdistance.eval(out[i], gts)\n",
    "            count += 1\n",
    "        iterator.set_description(\"acc: {0:.4f}; avg_ed: {0:.4f}\".format(tp / count, avg_ed / count))\n",
    "\n",
    "    acc = tp / count\n",
    "    avg_ed = avg_ed / count\n",
    "    return acc, avg_ed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 训练代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/nas/workspace/jupyter/download/mchar_data_list\n"
     ]
    }
   ],
   "source": [
    "cd mchar_data_list/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o -q mchar_train.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o -q mchar_val.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o -q mchar_test_a.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/nas/workspace/jupyter/download\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import click\n",
    "import string\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# from warpctc_pytorch import CTCLoss\n",
    "from torch.nn import CTCLoss\n",
    "\n",
    "train_json = json.load(open('./mchar_data_list/mchar_train.json'))\n",
    "train_label = [train_json[x]['label'] for x in train_json.keys()]\n",
    "train_path = ['./mchar_data_list/mchar_train' + x for x in train_json.keys()]\n",
    "\n",
    "val_json = json.load(open('./mchar_data_list/mchar_val.json'))\n",
    "val_label = [val_json[x]['label'] for x in val_json.keys()]\n",
    "val_path = ['./mchar_data_list/mchar_val' + x for x in val_json.keys()]\n",
    "\n",
    "\n",
    "def main(\n",
    "        abc='0123456789', \n",
    "         seq_proj=\"7x30\", \n",
    "         backend=\"resnet18\",\n",
    "         snapshot=None, \n",
    "         input_size=\"200x100\",\n",
    "         base_lr=1e-3,\n",
    "         step_size=1000,\n",
    "         max_iter=10000,\n",
    "         batch_size=20,\n",
    "         output_dir='./',\n",
    "         test_epoch=1,\n",
    "         test_init=None, \n",
    "         gpu='0'):\n",
    "    \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
    "    cuda = True if gpu is not '' else False\n",
    "\n",
    "    input_size = [int(x) for x in input_size.split('x')]\n",
    "    transform = Compose([\n",
    "        Rotation(),\n",
    "        Translation(),\n",
    "        # Scale(),\n",
    "        Resize(size=(input_size[0], input_size[1]))\n",
    "    ])\n",
    "    \n",
    "    data = TextDataset(train_path, train_label, transform=transform)\n",
    "    data_val = TextDataset(val_path, val_label, transform=transform)\n",
    "    \n",
    "    seq_proj = [int(x) for x in seq_proj.split('x')]\n",
    "    net = load_model(data.get_abc(), seq_proj, backend, snapshot, cuda)\n",
    "    optimizer = optim.Adam(net.parameters(), lr = base_lr, weight_decay=0.0001)\n",
    "    lr_scheduler = StepLR(optimizer, step_size=step_size, max_iter=max_iter)\n",
    "    loss_function = CTCLoss(zero_infinity = True)\n",
    "\n",
    "    acc_best = 0\n",
    "    epoch_count = 0\n",
    "    while True:\n",
    "        if (test_epoch is not None and epoch_count != 0 and epoch_count % test_epoch == 0) or (test_init and epoch_count == 0):\n",
    "            print(\"Test phase\")\n",
    "            data.set_mode(\"test\")\n",
    "            net = net.eval()\n",
    "            acc, avg_ed = test(net, data_val, data.get_abc(), cuda, 50)\n",
    "            net = net.train()\n",
    "            data.set_mode(\"train\")\n",
    "            if acc > acc_best:\n",
    "                if output_dir is not None:\n",
    "                    torch.save(net.state_dict(), os.path.join(output_dir, \"crnn_\" + backend + \"_\" + str(data.get_abc()) + \"_best\"))\n",
    "                acc_best = acc\n",
    "            print(\"acc: {}\\tacc_best: {}; avg_ed: {}\".format(acc, acc_best, avg_ed))\n",
    "\n",
    "        data_loader = DataLoader(data, batch_size=batch_size, num_workers=1, shuffle=True, collate_fn=text_collate)\n",
    "        loss_mean = []\n",
    "        iterator = tqdm(data_loader)\n",
    "        iter_count = 0\n",
    "        for sample in iterator:\n",
    "            # for multi-gpu support\n",
    "            if sample[\"img\"].size(0) % len(gpu.split(',')) != 0:\n",
    "                continue\n",
    "            optimizer.zero_grad()\n",
    "            imgs = Variable(sample[\"img\"])\n",
    "            labels = Variable(sample[\"seq\"]).view(-1)\n",
    "            label_lens = Variable(sample[\"seq_len\"].int())\n",
    "            if cuda:\n",
    "                imgs = imgs.cuda()\n",
    "            preds = net(imgs).cpu()\n",
    "            pred_lens = Variable(Tensor([preds.size(0)] * batch_size).int())\n",
    "            \n",
    "            # print(preds.shape, labels.shape)\n",
    "            loss = loss_function(preds, labels, pred_lens, label_lens)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm(net.parameters(), 10.0)\n",
    "            loss_mean.append(loss.item())\n",
    "            status = \"epoch: {}; iter: {}; lr: {}; loss_mean: {}; loss: {}\".format(epoch_count, lr_scheduler.last_iter, lr_scheduler.get_lr(), np.mean(loss_mean), loss.item())\n",
    "            iterator.set_description(status)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            iter_count += 1\n",
    "        if output_dir is not None:\n",
    "            torch.save(net.state_dict(), os.path.join(output_dir, \"crnn_\" + backend + \"_\" + str(data.get_abc()) + \"_last\"))\n",
    "        epoch_count += 1\n",
    "\n",
    "    return 1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 预测代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "test_path = glob.glob('./mchar_data_list/mchar_test_a/*')\n",
    "test_label = [[1]] * len(test_path)\n",
    "test_path.sort()\n",
    "\n",
    "def predict(net, data, abc, cuda, visualize, batch_size=50):\n",
    "    data_loader = DataLoader(data, batch_size=batch_size, num_workers=4, shuffle=False, collate_fn=text_collate)\n",
    "\n",
    "    count = 0\n",
    "    tp = 0\n",
    "    avg_ed = 0\n",
    "    out = []\n",
    "    iterator = tqdm_notebook(data_loader)\n",
    "    for sample in iterator:\n",
    "        imgs = Variable(sample[\"img\"])\n",
    "        if cuda:\n",
    "            imgs = imgs.cuda()\n",
    "        out += net(imgs, decode=True)\n",
    "        # print(out)\n",
    "        # break\n",
    "    return out\n",
    "\n",
    "model = load_model('0123456789', seq_proj=[7, 30], backend='resnet18', snapshot='crnn_resnet18_0123456789_best', cuda=True)\n",
    "\n",
    "transform = Compose([\n",
    "    # Rotation(), \n",
    "    # Translation(),\n",
    "    # Scale(),\n",
    "    Resize(size=(200, 100))\n",
    "    ])\n",
    "test_data = TextDataset(test_path, test_label, transform=transform)\n",
    "\n",
    "model.training = False\n",
    "test_predict = predict(model, test_data, '0123456789', True, False, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_submit = pd.read_csv('./mchar_data_list/mchar_sample_submit_A.csv')\n",
    "df_submit['file_code'] = test_predict\n",
    "df_submit.to_csv('submit.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作业\n",
    "改用YOLO系列的模型进行目标检测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "tianchi_metadata": {
   "competitions": [],
   "datasets": [
    {
     "id": "63734",
     "title": "零基础入门CV数据集"
    }
   ],
   "description": "",
   "notebookId": "162696",
   "source": "dsw"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
