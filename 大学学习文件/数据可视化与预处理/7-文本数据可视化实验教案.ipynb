{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cae6c5e",
   "metadata": {},
   "source": [
    "# <font face = \"微软雅黑\" color = blue size = 6>实验名称</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20103a09",
   "metadata": {},
   "source": [
    "## 文本数据可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747b14a0",
   "metadata": {},
   "source": [
    "# <font face = \"微软雅黑\" color = blue size = 6>实验目的</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de67b57c",
   "metadata": {},
   "source": [
    "## 通过该实验的实践，要求学生可以理解数据部分与整体的关系，掌握matplotlib和pyecharts的使用过程，能够熟练绘制常见的适合展示比例数据的图表。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43705c26",
   "metadata": {},
   "source": [
    "# <font face = \"微软雅黑\" color = blue size = 6>实验背景</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801698f5",
   "metadata": {},
   "source": [
    "## 比例数据是在日常生活和社会工作中十分常见的一种数据，它是通过部分占整体的比例来表示整体与部分，部分与部分之间关系，比如商业活动中，各细分市场的销量占比数据； 全国各省份的GDP排行情况等。对比例数据进行可视化展示，有很强的实用性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141a9193",
   "metadata": {},
   "source": [
    "# <font face = \"微软雅黑\" color = blue size = 6>实验原理</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4918d4f1",
   "metadata": {},
   "source": [
    "## 饼图：展示各类的比例分布\n",
    "## 环形图：展示各类的比例分布并增加标签说明\n",
    "## 百分比堆叠柱状图：展示多个不同类别下的相同子类的占比情况\n",
    "## 百分比堆叠面积图：展示多个类别随时间变化的比例变化趋势\n",
    "## 矩形树图：通过面积差异展示不同分类的比例差异，通过交互可以下钻到二级分类\n",
    "## 华夫饼图：通过单位矩形个数展示不同类别比例差异"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d7b408",
   "metadata": {},
   "source": [
    "# <font face = \"微软雅黑\" color = blue size = 6>实验环境</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e2fc5f",
   "metadata": {},
   "source": [
    "## Python 3.6.5\n",
    "\n",
    "## pyecharts 0.5.11\n",
    "\n",
    "## matplotlib 3.2.0\n",
    "\n",
    "## pandas 1.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cefbd1",
   "metadata": {},
   "source": [
    "# <font face = \"微软雅黑\" color = blue size = 6>实验步骤</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4990ba34",
   "metadata": {},
   "source": [
    "## 2 环境准备，安装所需的库"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059b3696",
   "metadata": {},
   "source": [
    "## 2 安装pyecharts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3133e210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyecharts==0.5.11\n",
      "  Using cached pyecharts-0.5.11-py2.py3-none-any.whl (122 kB)\n",
      "Requirement already satisfied: future in d:\\programfiles\\anaconda3\\lib\\site-packages (from pyecharts==0.5.11) (0.18.2)\n",
      "Collecting jupyter-echarts-pypkg==0.1.2\n",
      "  Using cached jupyter_echarts_pypkg-0.1.2-py3-none-any.whl\n",
      "Requirement already satisfied: lml==0.0.2 in d:\\programfiles\\anaconda3\\lib\\site-packages (from pyecharts==0.5.11) (0.0.2)\n",
      "Requirement already satisfied: pillow in d:\\programfiles\\anaconda3\\lib\\site-packages (from pyecharts==0.5.11) (9.0.1)\n",
      "Requirement already satisfied: jinja2 in d:\\programfiles\\anaconda3\\lib\\site-packages (from pyecharts==0.5.11) (2.11.3)\n",
      "Collecting pyecharts-javascripthon==0.0.6\n",
      "  Using cached pyecharts_javascripthon-0.0.6-py2.py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: pyecharts-jupyter-installer==0.0.3 in d:\\programfiles\\anaconda3\\lib\\site-packages (from jupyter-echarts-pypkg==0.1.2->pyecharts==0.5.11) (0.0.3)\n",
      "Collecting javascripthon>=0.10\n",
      "  Using cached javascripthon-0.12-py3-none-any.whl (526 kB)\n",
      "Collecting dukpy\n",
      "  Using cached dukpy-0.2.3.tar.gz (1.9 MB)\n",
      "Requirement already satisfied: setuptools in d:\\programfiles\\anaconda3\\lib\\site-packages (from javascripthon>=0.10->pyecharts-javascripthon==0.0.6->pyecharts==0.5.11) (61.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\programfiles\\anaconda3\\lib\\site-packages (from jinja2->pyecharts==0.5.11) (2.0.1)\n",
      "Building wheels for collected packages: dukpy\n",
      "  Building wheel for dukpy (setup.py): started\n",
      "  Building wheel for dukpy (setup.py): finished with status 'done'\n",
      "  Created wheel for dukpy: filename=dukpy-0.2.3-cp39-cp39-win_amd64.whl size=1262292 sha256=1609f947e7b557840389fe596c15159946228050b8dec8c4b600bd3a3378703c\n",
      "  Stored in directory: c:\\users\\lenov\\appdata\\local\\pip\\cache\\wheels\\68\\81\\1c\\587c6743986b3153a950f942e68a487e6c5a8873aa06680262\n",
      "Successfully built dukpy\n",
      "Installing collected packages: dukpy, javascripthon, pyecharts-javascripthon, jupyter-echarts-pypkg, pyecharts\n",
      "Successfully installed dukpy-0.2.3 javascripthon-0.12 jupyter-echarts-pypkg-0.1.2 pyecharts-0.5.11 pyecharts-javascripthon-0.0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install pyecharts==0.5.11 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cd52718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyecharts_snapshot in d:\\programfiles\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: pyppeteer>=0.0.25 in d:\\programfiles\\anaconda3\\lib\\site-packages (from pyecharts_snapshot) (1.0.2)\n",
      "Requirement already satisfied: pillow in d:\\programfiles\\anaconda3\\lib\\site-packages (from pyecharts_snapshot) (9.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in d:\\programfiles\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.25->pyecharts_snapshot) (4.11.3)\n",
      "Requirement already satisfied: pyee<9.0.0,>=8.1.0 in d:\\programfiles\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.25->pyecharts_snapshot) (8.2.2)\n",
      "Requirement already satisfied: certifi>=2021 in d:\\programfiles\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.25->pyecharts_snapshot) (2021.10.8)\n",
      "Requirement already satisfied: websockets<11.0,>=10.0 in d:\\programfiles\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.25->pyecharts_snapshot) (10.3)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in d:\\programfiles\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.25->pyecharts_snapshot) (1.4.4)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in d:\\programfiles\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.25->pyecharts_snapshot) (1.26.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in d:\\programfiles\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.25->pyecharts_snapshot) (4.64.0)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\programfiles\\anaconda3\\lib\\site-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.25->pyecharts_snapshot) (3.7.0)\n",
      "Requirement already satisfied: colorama in d:\\programfiles\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.42.1->pyppeteer>=0.0.25->pyecharts_snapshot) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyecharts_snapshot --proxy http://127.0.0.1:10901"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe607c2",
   "metadata": {},
   "source": [
    "## 3 绘制词云"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ec76e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyecharts import WordCloud\n",
    "#引入词云库\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "name = [\n",
    "    'Sam S Club', 'Macys', 'Amy Schumer', 'Jurassic World', 'Charter Communications',\n",
    "    'Chick Fil A', 'Planet Fitness', 'Pitch Perfect', 'Express', 'Home', 'Johnny Depp',\n",
    "    'Lena Dunham', 'Lewis Hamilton', 'KXAN', 'Mary Ellen Mark', 'Farrah Abraham',\n",
    "    'Rita Ora', 'Serena Williams', 'NCAA baseball tournament', 'Point Break']\n",
    "value = [\n",
    "    10000, 6181, 4386, 4055, 2467, 2244, 1898, 1484, 1112,\n",
    "    965, 847, 582, 555, 550, 462, 366, 360, 282, 273, 265]\n",
    "\"\"\"\n",
    "#上面代码是示例数据，在注释中，对程序没有影响\n",
    "post_data = pd.read_csv('./post_data.csv') #读取数据\n",
    "wordcloud = WordCloud(width=1300, height=620) #设置词云图片的宽高\n",
    "#wordcloud.add(\"\", name, value, word_size_range=[20, 100]) #设置词显示的大小范围\n",
    "post_data2=post_data.groupby(by=['category']).agg({'views':sum}).reset_index() #数据分类，聚合，然后重新排序\n",
    "#print(post_data2.info())\n",
    "wordcloud.add(\"\", post_data2['category'], post_data2['views'], word_size_range=[20, 100]) #创建词云\n",
    "wordcloud.render(path=\"./词云.html\")#生成html文件，在该python同级目录下"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0d5806",
   "metadata": {},
   "source": [
    "## 4 主题河流"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f07e8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyecharts import ThemeRiver\n",
    "#引入主题河流图\n",
    "\n",
    "#准备数据\n",
    "data = [\n",
    "    ['2015/11/08', 10, '分支1'], ['2015/11/09', 15, '分支1'], ['2015/11/10', 35, '分支1'],\n",
    "    ['2015/11/14', 7, '分支1'], ['2015/11/15', 2, '分支1'], ['2015/11/16', 17, '分支1'],\n",
    "    ['2015/11/17', 33, '分支1'], ['2015/11/18', 40, '分支1'], ['2015/11/19', 32, '分支1'],\n",
    "    ['2015/11/20', 26, '分支1'], ['2015/11/21', 35, '分支1'], ['2015/11/22', 40, '分支1'],\n",
    "    ['2015/11/23', 32, '分支1'], ['2015/11/24', 26, '分支1'], ['2015/11/25', 22, '分支1'],\n",
    "    ['2015/11/08', 35, '分支2'], ['2015/11/09', 36, '分支2'], ['2015/11/10', 37, '分支2'],\n",
    "    ['2015/11/11', 22, '分支2'], ['2015/11/12', 24, '分支2'], ['2015/11/13', 26, '分支2'],\n",
    "    ['2015/11/14', 34, '分支2'], ['2015/11/15', 21, '分支2'], ['2015/11/16', 18, '分支2'],\n",
    "    ['2015/11/17', 45, '分支2'], ['2015/11/18', 32, '分支2'], ['2015/11/19', 35, '分支2'],\n",
    "    ['2015/11/20', 30, '分支2'], ['2015/11/21', 28, '分支2'], ['2015/11/22', 27, '分支2'],\n",
    "    ['2015/11/23', 26, '分支2'], ['2015/11/24', 15, '分支2'], ['2015/11/25', 30, '分支2'],\n",
    "    ['2015/11/26', 35, '分支2'], ['2015/11/27', 42, '分支2'], ['2015/11/28', 42, '分支2'],\n",
    "    ['2015/11/08', 21, '分支3'], ['2015/11/09', 25, '分支3'], ['2015/11/10', 27, '分支3'],\n",
    "    ['2015/11/11', 23, '分支3'], ['2015/11/12', 24, '分支3'], ['2015/11/13', 21, '分支3'],\n",
    "    ['2015/11/14', 35, '分支3'], ['2015/11/15', 39, '分支3'], ['2015/11/16', 40, '分支3'],\n",
    "    ['2015/11/17', 36, '分支3'], ['2015/11/18', 33, '分支3'], ['2015/11/19', 43, '分支3'],\n",
    "    ['2015/11/20', 40, '分支3'], ['2015/11/21', 34, '分支3'], ['2015/11/22', 28, '分支3'],\n",
    "    ['2015/11/14', 7, '分支4'], ['2015/11/15', 2, '分支4'], ['2015/11/16', 17, '分支4'],\n",
    "    ['2015/11/17', 33, '分支4'], ['2015/11/18', 40, '分支4'], ['2015/11/19', 32, '分支4'],\n",
    "    ['2015/11/20', 26, '分支4'], ['2015/11/21', 35, '分支4'], ['2015/11/22', 40, '分支4'],\n",
    "    ['2015/11/23', 32, '分支4'], ['2015/11/24', 26, '分支4'], ['2015/11/25', 22, '分支4'],\n",
    "    ['2015/11/26', 16, '分支4'], ['2015/11/27', 22, '分支4'], ['2015/11/28', 10, '分支4'],\n",
    "    ['2015/11/08', 10, '分支5'], ['2015/11/09', 15, '分支5'], ['2015/11/10', 35, '分支5'],\n",
    "    ['2015/11/11', 38, '分支5'], ['2015/11/12', 22, '分支5'], ['2015/11/13', 16, '分支5'],\n",
    "    ['2015/11/14', 7, '分支5'], ['2015/11/15', 2, '分支5'], ['2015/11/16', 17, '分支5'],\n",
    "    ['2015/11/17', 33, '分支5'], ['2015/11/18', 40, '分支5'], ['2015/11/19', 32, '分支5'],\n",
    "    ['2015/11/20', 26, '分支5'], ['2015/11/21', 35, '分支5'], ['2015/11/22', 4, '分支5'],\n",
    "    ['2015/11/23', 32, '分支5'], ['2015/11/24', 26, '分支5'], ['2015/11/25', 22, '分支5'],\n",
    "    ['2015/11/26', 16, '分支5'], ['2015/11/27', 22, '分支5'], ['2015/11/28', 10, '分支5'],\n",
    "    ['2015/11/08', 10, '分支6'], ['2015/11/09', 15, '分支6'], ['2015/11/10', 35, '分支6'],\n",
    "    ['2015/11/11', 38, '分支6'], ['2015/11/12', 22, '分支6'], ['2015/11/13', 16, '分支6'],\n",
    "    ['2015/11/14', 7, '分支6'], ['2015/11/15', 2, '分支6'], ['2015/11/16', 17, '分支6'],\n",
    "    ['2015/11/17', 33, '分支6'], ['2015/11/18', 4, '分支6'], ['2015/11/19', 32, '分支6'],\n",
    "    ['2015/11/20', 26, '分支6'], ['2015/11/21', 35, '分支6'], ['2015/11/22', 40, '分支6'],\n",
    "    ['2015/11/23', 32, '分支6'], ['2015/11/24', 26, '分支6'], ['2015/11/25', 22, '分支6']\n",
    "]\n",
    "colors_list=['#FFA07A','#32CD32','#4169E1','#FAA460','#F0E68C','#8c564b','#e377c2','#7f7f7f','#bcbd22','#17becf']#备用颜色列表\n",
    "tr = ThemeRiver(\"主题河流图示例图\") #创建主题河流图\n",
    "tr.add(['分支1', '分支2', '分支3', '分支4', '分支5', '分支6'], data, is_label_show=False,label_color=colors_list) #添加数据\n",
    "tr.render(path=\"./主题河流.html\")#生成html，在该python文件的同级目录下"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846aafcb",
   "metadata": {},
   "source": [
    "## 5 关系图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dddd1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyecharts import Graph\n",
    "import os\n",
    "#引入json处理库\n",
    "import json\n",
    "\n",
    "#读取weibo-2.json，以utf-8的形式\n",
    "with open(os.path.join(\"./\", \"weibo-2.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    j = json.load(f) #解析json文件\n",
    "    #print(j)\n",
    "    nodes, links, categories, cont, mid, userl = j #将json数据解压赋值给左边的便利\n",
    "    #print(mid)\n",
    "graph = Graph(\"微博转发关系图\", width=1200, height=600) #创建图，设置宽高\n",
    "graph.add(\n",
    "    \"\",\n",
    "    nodes,\n",
    "    links,\n",
    "    categories,\n",
    "    label_pos=\"right\",#标签位置\n",
    "    graph_repulsion=50,#节点之间的斥力因子。默认为 50，值越大则斥力越大\n",
    "    is_legend_show=False,#不展示图例\n",
    "    line_curve=0.2,#线的曲度\n",
    "    label_text_color=None,#不设置标签字体颜色\n",
    ")\n",
    "graph.render(path=\"./关系图.html\")#生成html，在该python文件同级目录下"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607e4c5b",
   "metadata": {},
   "source": [
    "## 6 数据的获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b31c406f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['西线无战事/新西线无战事/AllQuietontheWesternFront', 'https://movie.douban.com/subject/3042261/', '2022-09-12(多伦多电影节)/2022-09-29(德国)/2022-10-28(德国网络)/费利克斯·卡米勒/阿尔布雷希特·舒赫/亚伦·希尔默/丹尼尔·布鲁赫/塞巴斯蒂安·胡克/安东·范·卢克/大卫·史崔梭德/埃丁·哈萨诺维奇/乔·温特劳布/吕克·费特...', '8.6(50025人评价)'], ['危笑/微笑(台)/魅笑(港)', 'https://movie.douban.com/subject/35779959/', '2022-09-22(奇幻影展)/2022-09-30(美国)/索茜·贝肯/凯尔·加尔纳/凯特琳·斯塔西/罗宾·薇格特/杰西·厄舍/卡尔·潘/朱迪·雷耶斯/凯文·凯皮/罗布·摩根/吉莲·珍塞尔/马蒂·马图利斯/朵拉·济丝/莎拉·卡普纳/马修·兰姆...', '6.3(16267人评价)'], ['野蛮人/宿劫(台)', 'https://movie.douban.com/subject/35730910/', '2022-08-29(FrightFest)/2022-09-09(美国)/乔治娜·坎贝尔/比尔·斯卡斯加德/贾斯汀·朗/马修·帕特里克·戴维斯/理查德·布雷克/库尔特·布劳诺勒/杰米斯·巴特勒/索菲·索伦森/雷切尔·福勒/J·R·埃斯波西托/凯特·尼科尔斯...', '6.9(18278人评价)'], ['共助2：国际/机密同盟2(台)/共助2：国际化', 'https://movie.douban.com/subject/35240874/', '2022-09-07(韩国)/玄彬/柳海真/林允儿/丹尼尔·海尼/陈善圭/郑允荷/林成宰/张荣男/韩国/梨晳勳/128分钟/共助2：国际/动作/李石勋Seok-hoonLee/韩语', '7.2(12348人评价)'], ['6/45/乐透大作战(台)/军旅六合彩', 'https://movie.douban.com/subject/35441582/', '2022-08-24(韩国)/高庚杓/李伊庚/音文硕/朴世婉/郭东延/李俊赫/南道润/韩国/朴奎泰/113分钟/6/45/剧情/喜剧/朴奎泰Gyu-taePark/韩语', '7.7(18895人评价)'], ['珀尔/“X”前传电影/欲珠', 'https://movie.douban.com/subject/35801819/', '2022-09-03(威尼斯电影节)/2022-09-16(美国)/米娅·高斯/坦蒂·莱特/马修·桑德兰/大卫·科伦斯韦/艾玛·詹金斯·普罗/阿利斯泰尔·休厄尔/艾米莉亚·瑞德/ToddRippon/美国/加拿大/缇·威斯特/102分钟/珀尔/恐怖/缇·威斯特...', '7.3(18896人评价)'], ['狼狩猎/行动代号：狼狩猎/ProjectWolfHunting', 'https://movie.douban.com/subject/35426373/', '2022-09-10(多伦多电影节)/2022-09-21(韩国)/徐仁国/张东润/郑素敏/成东日/朴浩山/张荣男/金灿亨/韩国/金泓善/122分钟/狼狩猎/动作/惊悚/金洪宣Hong-sunKim/韩语', '5.9(8083人评价)'], ['良心护士/好护士/死亡天使(台)', 'https://movie.douban.com/subject/26641055/', '2022-09-11(多伦多电影节)/2022-10-19(美国)/2022-10-26(美国网络)/杰西卡·查斯坦/埃迪·雷德梅恩/纳马迪·阿斯穆格哈/金·迪肯斯/马利克·约巴/阿利克斯·韦斯特·莱弗勒/诺亚·艾默里奇/阿杰·奈杜/玛西娅·让·库尔茨/德温·麦克道维尔...', '7.0(7788人评价)'], ['地狱尖兵/Luchshiyevadu/地狱之最', 'https://movie.douban.com/subject/36118263/', '2022-10-05(俄罗斯)/阿列克谢·克拉夫琴科/米哈伊尔·波格丹诺夫/彼得·哈尔琴科/谢尔盖·加鲁索夫/乔治·博洛涅夫/安东·巴格梅特/谢尔盖·贝斯帕洛夫/亚历山大·贝斯梅特尼/扎哈尔·切列佐夫/叶夫根尼·克梅连科/谢尔盖·丹尼洛夫...', '8.2(4037人评价)'], ['哥们儿/兄弟/哥儿们(台)', 'https://movie.douban.com/subject/35017155/', '2022-09-10(多伦多电影节)/2022-09-30(美国)/比利·艾希纳/卢克·马可法莱恩/盖伊·布兰纳姆/密斯·劳伦斯/茨·麦迪逊/多特-玛丽·琼斯/吉姆·拉什/伊芙·林德利/莫妮卡·雷蒙德/古列雷莫·迪亚兹/杰·罗德里格斯/阿曼达·比尔斯...', '7.3(6781人评价)']]\n"
     ]
    }
   ],
   "source": [
    "#爬取豆瓣电影推荐清单\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers={'user-agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36'}\n",
    "\n",
    "res_movies = requests.get('https://movie.douban.com/chart', headers=headers)\n",
    "bs_movies = BeautifulSoup(res_movies.text,'html.parser')\n",
    "list_movies= bs_movies.find_all('div',class_='pl2')\n",
    "list_all = []\n",
    "for movie in list_movies:\n",
    "    tag_a = movie.find('a')\n",
    "    name = tag_a.text.replace(' ', '').replace('\\n', '')\n",
    "    # 电影名，使用replace方法去掉多余的空格及换行符\n",
    "    url = tag_a['href']\n",
    "    # 电影详情页的链接\n",
    "    tag_p = movie.find('p', class_='pl')\n",
    "    # 提取父级标签中的<p>标签\n",
    "    information = tag_p.text.replace(' ', '').replace('\\n', '')\n",
    "    # 电影基本信息，使用replace方法去掉多余的空格及换行符\n",
    "    tag_div = movie.find('div', class_='star clearfix')\n",
    "    # 提取父级标签中的<div>标签\n",
    "    rating = tag_div.text.replace(' ', '').replace('\\n', '')\n",
    "    # 电影评分信息，使用replace方法去掉多余的空格及换行符\n",
    "    list_all.append([name,url,information,rating])\n",
    "    # 将电影名、URL、电影基本信息和电影评分信息，封装为列表，用append方法添加进list_all\n",
    "print(list_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79334d2",
   "metadata": {},
   "source": [
    "## 7 文本数据的预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b861288f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jieba in d:\\programfiles\\anaconda3\\lib\\site-packages (0.42.1)\n",
      "Requirement already satisfied: paddlepaddle in d:\\programfiles\\anaconda3\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: Pillow in d:\\programfiles\\anaconda3\\lib\\site-packages (from paddlepaddle) (9.0.1)\n",
      "Requirement already satisfied: paddle-bfloat==0.1.7 in d:\\programfiles\\anaconda3\\lib\\site-packages (from paddlepaddle) (0.1.7)\n",
      "Requirement already satisfied: opt-einsum==3.3.0 in d:\\programfiles\\anaconda3\\lib\\site-packages (from paddlepaddle) (3.3.0)\n",
      "Requirement already satisfied: decorator in d:\\programfiles\\anaconda3\\lib\\site-packages (from paddlepaddle) (5.1.1)\n",
      "Requirement already satisfied: numpy>=1.13 in d:\\programfiles\\anaconda3\\lib\\site-packages (from paddlepaddle) (1.21.0)\n",
      "Requirement already satisfied: protobuf<=3.20.0,>=3.1.0 in d:\\programfiles\\anaconda3\\lib\\site-packages (from paddlepaddle) (3.19.1)\n",
      "Requirement already satisfied: requests>=2.20.0 in d:\\programfiles\\anaconda3\\lib\\site-packages (from paddlepaddle) (2.27.1)\n",
      "Requirement already satisfied: astor in d:\\programfiles\\anaconda3\\lib\\site-packages (from paddlepaddle) (0.8.1)\n",
      "Requirement already satisfied: six in d:\\programfiles\\anaconda3\\lib\\site-packages (from paddlepaddle) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\programfiles\\anaconda3\\lib\\site-packages (from requests>=2.20.0->paddlepaddle) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programfiles\\anaconda3\\lib\\site-packages (from requests>=2.20.0->paddlepaddle) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programfiles\\anaconda3\\lib\\site-packages (from requests>=2.20.0->paddlepaddle) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\programfiles\\anaconda3\\lib\\site-packages (from requests>=2.20.0->paddlepaddle) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install jieba\n",
    "!pip install paddlepaddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a5bf68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "paddle.enable_static()#启动飞桨深度学习框架"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb97af71",
   "metadata": {},
   "source": [
    "## 7.1 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7648b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paddle enabled successfully......\n",
      "DEBUG 2022-11-10 10:19:50,762 _compat.py:47] Paddle enabled successfully......\n",
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG 2022-11-10 10:19:51,366 __init__.py:113] Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paddle mode：我/来自/北京清华大学\n",
      "paddle mode：乒乓球/拍卖/完/了\n",
      "paddle mode：中国科学技术大学\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache C:\\Users\\lenov\\AppData\\Local\\Temp\\jieba.cache\n",
      "DEBUG 2022-11-10 10:19:51,943 __init__.py:146] Dumping model to file cache C:\\Users\\lenov\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.633 seconds.\n",
      "DEBUG 2022-11-10 10:19:52,002 __init__.py:164] Loading model cost 0.633 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "DEBUG 2022-11-10 10:19:52,003 __init__.py:166] Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全模式：我/来自/北京/清华/清华大学/华大/大学\n",
      "精确模式：我/来自/北京/清华大学\n",
      "搜索引擎模式：小明/硕士/毕业/于/中国/科学/学院/科学院/中国科学院/计算/计算所/，/后/在/日本/京都/大学/日本京都大学/深造\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "jieba.enable_paddle()##启动paddle模式，v0.40版本开始支持，早期版本不支持\n",
    "\n",
    "strs = [\"我来自北京清华大学\", \"乒乓球拍卖完了\", \"中国科学技术大学\"]\n",
    "for str in strs:\n",
    "    seg_list = jieba.cut(str, use_paddle = True) #应用paddle分词\n",
    "    print(\"paddle mode：\" + '/'.join(list(seg_list)))\n",
    "\n",
    "seg_list = jieba.cut(\"我来自北京清华大学\", cut_all = True) #应用全模式分词\n",
    "print(\"全模式：\" + '/'.join(list(seg_list)))\n",
    "\n",
    "seg_list = jieba.cut(\"我来自北京清华大学\", cut_all = False) #应用精确模式分词\n",
    "print(\"精确模式：\" + '/'.join(list(seg_list)))\n",
    "\n",
    "seg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\") #应用搜索引擎模式\n",
    "print(\"搜索引擎模式：\" + '/'.join(list(seg_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60a071d",
   "metadata": {},
   "source": [
    "## 7.2 使用用户词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "393e4ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "李小福/是/创新/办公室/主任/也/是/云/计算/方面/专家/；/什么/是/八一双鹿\n",
      "李/小/福/是/创新/办公室/主任/也/是/云/计算/方面/专家/；/什么/是/八一双鹿\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "jieba.load_userdict(\"./userdict.txt\")#导入自定义用户词典\n",
    "\n",
    "jieba.add_word('八一双鹿')\n",
    "\n",
    "str = \"李小福是创新办公室主任也是云计算方面专家；什么是八一双鹿\"\n",
    "seg_list = jieba.cut(str, cut_all = False)\n",
    "print('/'.join(list(seg_list)))\n",
    "\n",
    "jieba.del_word('李小福')\n",
    "seg_list = jieba.cut(str, cut_all = False)\n",
    "print('/'.join(list(seg_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b231f83",
   "metadata": {},
   "source": [
    "## 7.3 关键词提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60b2a769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "俄罗斯,武器,北约,弹药,提供,报道,玩火,军事援助,火箭筒,照会\n"
     ]
    }
   ],
   "source": [
    "#基于TF-IDF的关键词提取\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "content = open(\"./doc-tfidf.txt\", 'rb').read()\n",
    "\n",
    "tags = jieba.analyse.extract_tags(content, topK=10, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'))\n",
    "\"\"\"\n",
    "sentence, topK=20, withWeight=False, allowPOS=()\n",
    "sentence 为待提取的文本\n",
    "topK 为返回几个TF/IDF权重最大的关键词，默认为20个\n",
    "withWeight 为是否一并返回关键词权重值，默认值为False\n",
    "allowPOS 仅包括指定词性的词，默认值为空，即不做筛选\n",
    "\"\"\"\n",
    "print(\",\".join(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6da623b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "俄罗斯,武器,北约,弹药,提供,报道,玩火,军事援助,火箭筒,照会\n"
     ]
    }
   ],
   "source": [
    "#基于text-rank的关键词提取\n",
    "tags1 =  jieba.analyse.textrank(content, topK=10, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'))\n",
    "print(\",\".join(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea023cd",
   "metadata": {},
   "source": [
    "## 7.4 设置停止词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b6f5bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "美国国防部,武器,北约,弹药,提供,拉夫罗夫,8.2,这轮,NASAMS,15\n"
     ]
    }
   ],
   "source": [
    "content = open(\"./doc-tfidf.txt\", 'rb').read()\n",
    "jieba.analyse.set_stop_words(\"./stop_words.txt\")#导入停止词\n",
    "tags = jieba.analyse.extract_tags(content, topK=10)\n",
    "print(\",\".join(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a308b8",
   "metadata": {},
   "source": [
    "# <font face = \"微软雅黑\" color = blue size = 6>实验总结</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60a7b48",
   "metadata": {},
   "source": [
    "## 在实验中，通过实践练习，提高学生使用Matplotlib和pyecharts进行文本数据可视化的熟练程度，总结适合展示文本数据的图表，能够解释文本数据所包含的实际意义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c153f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
